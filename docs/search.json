[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Blog",
    "section": "",
    "text": "Inteligencia Artificial contra el Fraude: Análisis de transacciones de tarjetas de crédito\n\n\n\n\n\nAbordamos el desafío de predecir transacciones fraudulentas en tarjetas de crédito, analizando un dataset europeo de 2023. Aplicamos técnicas como PCA y Random Forest, y descubrimos que, a pesar de presentarse como real, el dataset fue generado artificialmente.\n\n\n\n\n\n\n2023-11-20\n\n\n\n\n\n\n\n\n\n\n\nTest de Hipótesis: explorando correlaciones en datos de seguros de salud\n\n\nSe utiliza como base el libro de Jim Fros: 'Hypothesis Testing: An Intuitive Guide for Making Data Driven Decisions'. Este análisis se enfoca en descubrir y entender las correlaciones subyacentes en los datos de seguros de salud, aplicando métodos de prueba de hipótesis para revelar tendencias y patrones significativos.\n\n\n\n`2023-11-14`{=html}\n\n\n\n\n\n\n\n\n\nTest de Hipótesis: Análisis de los 50 libros más vendidos en amazon (2009 - 2019)\n\n\nEste análisis profundiza en cómo el género literario influye en la valoración de los usuarios, el número de reseñas y el precio de los bestsellers en Amazon.\n\n\n\n`2023-10-16`{=html}\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-11-14-health_insurance_hypothesis_testing/health-Insurance_hypothesis_testing.html",
    "href": "posts/2023-11-14-health_insurance_hypothesis_testing/health-Insurance_hypothesis_testing.html",
    "title": "Test de Hipótesis: explorando correlaciones en datos de seguros de salud",
    "section": "",
    "text": "From Kaggle: Health Insurance dataset\n\n\nDataset de una asegurada de salud. En las aseguradoras, la capacidad de predecir los costes de cada cliente es crucial. En función de estas predicciones ajustaran el precio de la póliza y el beneficio que obtendrán.\n\n\n\nVamos a responder a las siguientes preguntas con validez estadística:\n\n¿Los fumadores generan más gasto a la aseguradora que los no fumadores?\n¿Las mujeres tienen un BMI diferente a los hombres?\n¿La proporción de fumadores es diferente según la región?\n¿El promedio de BMI en las mujeres es diferente según el número de hijos que tengan?\n¿Es diferente la proporción de fumadores en ambos sexos?\n\n\n\n\n\n\n\nMatemáticas y código: en Inglés\n\n\n\n\n\nAunque el texto principal está en español, los términos matemáticos y el código están en inglés.\nEsta práctica sigue el estándar internacional y ayuda a familiarizarse con el lenguaje técnico más utilizado en el campo de la ciencia de datos."
  },
  {
    "objectID": "posts/2023-11-14-health_insurance_hypothesis_testing/health-Insurance_hypothesis_testing.html#a.-entendimiento-del-negocio",
    "href": "posts/2023-11-14-health_insurance_hypothesis_testing/health-Insurance_hypothesis_testing.html#a.-entendimiento-del-negocio",
    "title": "Test de Hipótesis: explorando correlaciones en datos de seguros de salud",
    "section": "",
    "text": "From Kaggle: Health Insurance dataset\n\n\nDataset de una asegurada de salud. En las aseguradoras, la capacidad de predecir los costes de cada cliente es crucial. En función de estas predicciones ajustaran el precio de la póliza y el beneficio que obtendrán.\n\n\n\nVamos a responder a las siguientes preguntas con validez estadística:\n\n¿Los fumadores generan más gasto a la aseguradora que los no fumadores?\n¿Las mujeres tienen un BMI diferente a los hombres?\n¿La proporción de fumadores es diferente según la región?\n¿El promedio de BMI en las mujeres es diferente según el número de hijos que tengan?\n¿Es diferente la proporción de fumadores en ambos sexos?\n\n\n\n\n\n\n\nMatemáticas y código: en Inglés\n\n\n\n\n\nAunque el texto principal está en español, los términos matemáticos y el código están en inglés.\nEsta práctica sigue el estándar internacional y ayuda a familiarizarse con el lenguaje técnico más utilizado en el campo de la ciencia de datos."
  },
  {
    "objectID": "posts/2023-11-14-health_insurance_hypothesis_testing/health-Insurance_hypothesis_testing.html#b.-entendimiento-de-los-datos",
    "href": "posts/2023-11-14-health_insurance_hypothesis_testing/health-Insurance_hypothesis_testing.html#b.-entendimiento-de-los-datos",
    "title": "Test de Hipótesis: explorando correlaciones en datos de seguros de salud",
    "section": "1b. Entendimiento de los datos",
    "text": "1b. Entendimiento de los datos\nPara empezar, importamos las librerías que vamos a utilizar:\n\nPandas: Pandas es una biblioteca esencial en la ciencia de datos que proporciona estructuras de datos flexibles y eficientes, como DataFrames, para el análisis y manipulación de datos tabulares. Es ampliamente utilizada para limpiar, transformar y analizar datos, lo que la convierte en una herramienta fundamental para la preparación de datos en proyectos de ciencia de datos.\nScipy: Scipy es una biblioteca que se construye sobre NumPy y ofrece una amplia variedad de módulos y funciones especializadas para aplicaciones científicas y matemáticas. Incluye herramientas para estadísticas, optimización, álgebra lineal y procesamiento de señales, lo que la hace esencial en la investigación y el análisis de datos en ciencia de datos.\nNumpy: NumPy es esencial en ciencia de datos para operaciones numéricas eficientes. Su estructura de matriz multidimensional permite cálculos y análisis de datos, siendo clave en manipulación y modelado.\nStatsmodels: Statsmodels se centra en proporcionar herramientas y modelos estadísticos para el análisis de datos. La usaremos únicamente para realizar un Two-Sample Proportion Test.\nPlotly Express: Plotly Express es una biblioteca de visualización de datos que simplifica la creación de gráficos interactivos y visuales. Es especialmente útil en la exploración de datos y la comunicación de resultados en ciencia de datos, permitiendo a los científicos de datos crear visualizaciones informativas y atractivas con facilidad.\n\n\n\nCode\n# Import libraries\nimport pandas as pd\nfrom scipy import stats\nimport numpy as np\n\nimport statsmodels.api as sm\n\n# Import plotly and customize\nimport plotly.io as pio\nimport plotly.express as px\n\npio.templates.default = \"plotly\"\npio.templates[\"plotly\"].layout.colorway = px.colors.qualitative.Set2\n\n\nCargamos el dataset y describimos brevemente sus características.\n\ndf = pd.read_csv(\"health_insurance.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.92400\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.55230\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.46200\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.47061\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.85520\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCampo\nTipo de Dato\nDescripción\nEjemplo\n\n\n\n\nage\nNumérico (entero)\nEdad del asegurado\n29\n\n\nsex\nCategórico\nSexo del asegurado\nFemale/Male\n\n\nbmi\nNumérico\nÍndice de Masa Corporal del asegurado\n26.6\n\n\nchildren\nNumérico (entero)\nNúmero de hijos del asegurado\n2\n\n\nsmoker\nCategórico\nEstatus de fumador del asegurado\nNo/Yes\n\n\nregion\nCategórico\nRegión del asegurado\nSouthwest\n\n\ncharges\nNumérico (USD)\nCargos realizados a la compañía\n12345.67"
  },
  {
    "objectID": "posts/2023-11-14-health_insurance_hypothesis_testing/health-Insurance_hypothesis_testing.html#a.-preparación-de-los-datos",
    "href": "posts/2023-11-14-health_insurance_hypothesis_testing/health-Insurance_hypothesis_testing.html#a.-preparación-de-los-datos",
    "title": "Test de Hipótesis: explorando correlaciones en datos de seguros de salud",
    "section": "2a. Preparación de los datos",
    "text": "2a. Preparación de los datos\n\n2a.1 Typecasting\nComprobamos el tipo de datos de las columnas y los modificamos conforme nuestra descripción inicial.\n\ndf.dtypes\n\nage           int64\nsex          object\nbmi         float64\nchildren      int64\nsmoker       object\nregion       object\ncharges     float64\ndtype: object\n\n\n\ncategorical_columns = [\"sex\", \"smoker\", \"region\"]\ndf[categorical_columns] = df[categorical_columns].astype(\"category\")\n\n\ndf.dtypes\n\nage            int64\nsex         category\nbmi          float64\nchildren       int64\nsmoker      category\nregion      category\ncharges      float64\ndtype: object\n\n\n\n\n2a.2 Manejo de duplicados\n¿Cuantas filas son exactamente iguales?\n\ndf.duplicated().sum()\n\n1\n\n\n\ndf[df.duplicated(keep=False)]\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n195\n19\nmale\n30.59\n0\nno\nnorthwest\n1639.5631\n\n\n581\n19\nmale\n30.59\n0\nno\nnorthwest\n1639.5631\n\n\n\n\n\n\n\n\ndf[df[\"bmi\"] == 30.59]\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n195\n19\nmale\n30.59\n0\nno\nnorthwest\n1639.56310\n\n\n423\n25\nmale\n30.59\n0\nno\nnortheast\n2727.39510\n\n\n526\n19\nfemale\n30.59\n2\nno\nnorthwest\n24059.68019\n\n\n567\n41\nmale\n30.59\n2\nno\nnorthwest\n7256.72310\n\n\n581\n19\nmale\n30.59\n0\nno\nnorthwest\n1639.56310\n\n\n983\n27\nfemale\n30.59\n1\nno\nnortheast\n16796.41194\n\n\n1158\n20\nfemale\n30.59\n0\nno\nnortheast\n2459.72010\n\n\n\n\n\n\n\nVemos que las observaciones 195 y 581 tienen exactamente los mismos datos. Teniendo en cuenta que comparten el mismo “bmi” exacto con otras muchas observaciones. Vamos a valorar el duplicado como una casualidad. Por ello, vamos a mantenerlo en nuestro conjunto de datos.\n\n\n2a.3 Análisis de valores atípicos\nEn esta ocasión vamos a observar los outliers mediante dos métodos diferentes.\n\nUsando Z-Score\nUsando Z-Score: El Z-Score mide cuántas desviaciones estándar se encuentra un valor de la media. Valores que caen fuera de un rango de Z-Score específico se consideran outliers. Por ejemplo, un Z-Score de 2 indica que el valor está a dos desviaciones estándar de la media.\nEn general, un Z-Score de más de 3 se considera un outlier.\n\ndef identify_outliers(data, column_name, threshold=3):\n    z_scores = np.abs(stats.zscore(data[column_name]))\n    outliers = data[z_scores &gt; threshold]\n    return outliers\n\n\n# Identify outliers in 'age'\noutliers_age = identify_outliers(df, \"age\")\n\n# Identify outliers in 'bmi'\noutliers_bmi = identify_outliers(df, \"bmi\")\n\n# Identify outliers in 'charges'\noutliers_charges = identify_outliers(df, \"charges\")\n\nprint(\"Outliers in 'age':\")\nprint(outliers_age)\n\nprint(\"Outliers in 'bmi':\")\nprint(outliers_bmi)\n\nprint(\"Outliers in 'charges':\")\nprint(outliers_charges)\n\nOutliers in 'age':\nEmpty DataFrame\nColumns: [age, sex, bmi, children, smoker, region, charges]\nIndex: []\nOutliers in 'bmi':\n      age   sex    bmi  children smoker     region     charges\n116    58  male  49.06         0     no  southeast  11381.3254\n847    23  male  50.38         1     no  southeast   2438.0552\n1047   22  male  52.58         1    yes  southeast  44501.3982\n1317   18  male  53.13         0     no  southeast   1163.4627\nOutliers in 'charges':\n      age     sex     bmi  children smoker     region      charges\n34     28    male  36.400         1    yes  southwest  51194.55914\n543    54  female  47.410         0    yes  southeast  63770.42801\n577    31  female  38.095         1    yes  northeast  58571.07448\n819    33  female  35.530         0    yes  northwest  55135.40209\n1146   60    male  32.800         0    yes  southwest  52590.82939\n1230   52    male  34.485         3    yes  northwest  60021.39897\n1300   45    male  30.360         0    yes  southeast  62592.87309\n\n\n\n\nUsando boxplot y IQR\nIQR: El rango intercuartil (IQR) es la diferencia entre el tercer cuartil (Q3) y el primer cuartil (Q1). Los valores que caen por debajo de Q1 - 1.5 * IQR o por encima de Q3 + 1.5 * IQR se consideran outliers.\nLos valores fuera del de este rango, son los que se representa como puntos en los boxplots.\n\n# create a boxplot with 'age', 'bmi' y 'charges' variables\nselected_columns = [\"age\", \"bmi\", \"charges\"]\n\nfor column in selected_columns:\n    fig = px.box(df, x=column, title=f\"Boxplot of {column}\")\n    fig.update_layout(title_x=0.5)\n    fig.show()\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n\nZ-score vs Boxplot\nPodemos ver como boxplot señala a más observaciones como outliers. En el caso de “bmi” las duplica, mientras que en “charges” son más del doble los detectados. Esto nos recuerda que juzgar una muestra como atípica, depende de nuestro método.\n\n\nIdeas destacadas\nAmbos métodos coinciden en: - “age”: no se detectan outliers - “BMI”: existen outliers, personas con una alta obesidad - “charges”: existen outliers, personas con cargos muy altos al seguro\nSe pueden hacer tres cosas con los outliers, siguiendo la mnemotecnia 3R: rectificar, retener o remover. En este caso, podemos estar ante valores atípicos genuinos, por tanto vamos a retenerlos por el momento.\n\n\n\n2a.4 Variables con varianza cercana a cero\nVariables númericas\n\nselected_columns = [\"age\", \"bmi\", \"children\", \"charges\"]\n\n# Calculate the variance for each column\nvariances = df[selected_columns].var()\n\n# Define a threshold for variance\nthreshold = 0.1\n\n# Identify columns with near-zero or very small variance\nzero_variance_cols = variances[variances &lt;= threshold].index\n\nprint(\"Columns with Zero & Near Zero Variance:\")\nprint(zero_variance_cols)\n\nColumns with Zero & Near Zero Variance:\nIndex([], dtype='object')\n\n\nVariables categoricas\n\nselected_columns = [\"sex\", \"smoker\", \"region\"]\n\n# Calculate the proportion of the most common category in each column\nprop_most_common = df[selected_columns].apply(\n    lambda col: col.value_counts().max() / len(col)\n)\n\n# Define a threshold for the proportion\nthreshold = 1\n\n# Identify columns with the proportion of the most common category close to 1\nzero_variance_cols = prop_most_common[prop_most_common &gt;= threshold].index\n\nprint(\"Columns with Zero & Near Zero Proportion:\")\nprint(zero_variance_cols)\n\nColumns with Zero & Near Zero Proportion:\nIndex([], dtype='object')\n\n\n\nIdeas destacadas\nNo se detectan variables con Zero & Near Zero Variance. A priori, todas las variables pueden estar aportando información.\n\n\n\n2a.5 Valores ausentes o faltantes\n\ndf.isnull().sum()\n\nage         0\nsex         0\nbmi         0\nchildren    0\nsmoker      0\nregion      0\ncharges     0\ndtype: int64\n\n\n\nIdeas destacadas\n\nNo hay missing values"
  },
  {
    "objectID": "posts/2023-11-14-health_insurance_hypothesis_testing/health-Insurance_hypothesis_testing.html#pruebas-de-hipótesis",
    "href": "posts/2023-11-14-health_insurance_hypothesis_testing/health-Insurance_hypothesis_testing.html#pruebas-de-hipótesis",
    "title": "Test de Hipótesis: explorando correlaciones en datos de seguros de salud",
    "section": "4. Pruebas de hipótesis",
    "text": "4. Pruebas de hipótesis\nVamos a responder a las siguientes preguntas con validez estadística:\n\n¿Los fumadores generan más gasto a la aseguradora que los no fumadores?\n¿Las mujeres tienen un BMI diferente a los hombres?\n¿La proporción de fumadores es diferente según la región?\n¿El promedio de BMI en las mujeres es diferente según el número de hijos que tengan?\n¿Es diferente la proporción de fumadores en ambos sexos?\n\n\n4.1 ¿Los fumadores generan más gasto a la aseguradora que los no fumadores?\nPrimero hagamos una exploración visual.\n\nfig = px.violin(\n    df,\n    x=\"charges\",\n    color=\"smoker\",\n    box=True,\n    points=\"outliers\",\n)\n\nfig.update_layout(\n    title_text=\"Charges violin plot by smoker status\",\n)\n\nfig.show()\n\n\n                                                \n\n\nEn este caso las diferencias entre ambos grupos son obvias. Pero para poder afirmarlo adecuadamente, debemos hacerlo con significancia estadística. Vamos a comprobarlo con un test de hipótesis.\n\n1. Seleccionamos la hipótesis y el nivel de significancia\nH0: El gasto medio de los fumadores es igual al de los no-fumadores\nHa: El gasto medio de los fumadores es diferente al de los no-fumadores\nalpha = 0.05\n\n\n2. Identificamos el tipo de test\nDeseamos comparar las medias de dos grupos de nuestra sample. Por lo tanto, un 2-Sample t-Test parece ser adecuado. Lo primero es verificar si se cumplen los requisitos del test.\n\n2.1 Requisitos del test\n\nTenemos una sample representativa de la population.\nLos datos son continuos.\nLas muestras siguen una distribución normal o hay más de 15 observaciones.\nLos grupos son independientes.\nLas varianzas son iguales (o al menos similares).\n\nExaminemos nuestra sample para ver si podemos aplicar el test.\n\n\n\n\n\n\nLibro de refencia\n\n\n\n\n\nBook: Hypothesis Testing An Intuitive Guide For Making Data Driven Decisions\nPage: 48\nSection: 2-Sample t-Tests\n\n\n\nNúmero de observaciones para cada grupo:\n\ndf[\"smoker\"].value_counts()\n\nsmoker\nno     1064\nyes     274\nName: count, dtype: int64\n\n\nEn inspección visual de los datos, vemos que no siguen una distribución normal.\nEstudiemos la relación entre las varianzas de cada grupo.\n\nsmokers = df[df[\"smoker\"] == \"yes\"]\nnonsmokers = df[df[\"smoker\"] == \"no\"]\n\n\n# Perform the Levene's Test\nstatistic, p_value = stats.levene(smokers[\"charges\"], nonsmokers[\"charges\"])\n\n# Significance level\nalpha = 0.05\n\n# Check for significance\nif p_value &lt; alpha:\n    print(\"We reject the null hypothesis. The variances are not similar.\")\nelse:\n    print(\"We fail to reject the null hypothesis. The variances are similar.\")\n\nWe reject the null hypothesis. The variances are not similar.\n\n\nVolvamos sobre los requisitos del test:\n\n¿Tenemos una sample representativa de la population? Suponemos que sí.\n¿Los datos son continuos? Sí.\n¿Las muestras siguen una distribución normal o hay más de 15 observaciones?\n\nNo, las muestras no siguen una distribución normal. Pero hay más de 15 observaciones en cada grupo, gracias al teorema central del límite podemos renunciar al supuesto de normalidad.\n\n¿Los grupos son independientes? Sí.\n¿Las varianzas son iguales (o al menos similares)?\n\nNo.\n\n\nNo cumplen los requisitos para realizar un 2-Sample t-Test. Vamos a realizar un test tipo Welch’s t-test.\nPara realizar el test utilizaremos scipy.stats.ttest_ind. Es necesario definir el parámetro equal_varbool como False.\n\n# Perform the Welch's t-test (equal_var=False)\nstatistic, p_value = stats.ttest_ind(\n    nonsmokers[\"charges\"], smokers[\"charges\"], equal_var=False\n)\n\n# Print the results\nprint(\"p-value:\", p_value)\n\n# Check for significance\nalpha = 0.05  # Significance level\nif p_value &lt; alpha:\n    print(\n        \"We reject the null hypothesis. There are significant differences between the groups.\"\n    )\nelse:\n    print(\n        \"We fail to reject the null hypothesis. There are no significant differences between the groups.\"\n    )\n\np-value: 5.88946444671698e-103\nWe reject the null hypothesis. There are significant differences between the groups.\n\n\n\n\n\nIdeas destacadas\nComo ya nos adelantaba la exploración visual, rechazamos la hipótesis nula. Tenemos evidencia suficiente (95% y 99%) para demostrar que existe una diferencia en los cargos de fumadores y no fumadores.\n\n\n\n4.2 ¿Las mujeres tienen un BMI diferente a los hombres?\nPrimero hagamos una exploración visual.\n\nfig = px.violin(\n    df,\n    x=\"bmi\",\n    color=\"sex\",\n    box=True,\n    points=\"outliers\",\n)\n\nfig.update_layout(\n    title_text=\"Charges violin plot by smoker status\",\n)\n\nfig.show()\n\n\n                                                \n\n\nEn esta ocasión “bmi” podría no variar en función de “sex”. ¿Pero es estadísticamente significativo? Vamos a comprobarlo con un test de hipótesis.\n\n1. Seleccionamos la hipótesis y el nivel de significancia\nH0: El BMI medio de las mujeres es igual al de los hombres\nHa: El BMI medio de las mujeres es diferente al de los hombres\nalpha = 0.05\n\n\n2. Identificamos el tipo de test\nDeseamos comparar las medias de dos grupos de nuestra sample. Por lo tanto, un 2-Sample t-Test parece ser adecuado. Lo primero es verificar si se cumplen los requisitos del test.\n\n2.1 Requisitos del test\n\nTenemos una sample representativa de la population.\nLos datos son continuos.\nLas muestras siguen una distribución normal o hay más de 15 observaciones.\nLos grupos son independientes.\nLas varianzas son iguales (o al menos similares).\n\nExaminemos nuestra sample para ver si podemos aplicar el test.\n\n\n\n\n\n\nLibro de refencia\n\n\n\n\n\nBook: Hypothesis Testing An Intuitive Guide For Making Data Driven Decisions\nPage: 48\nSection: 2-Sample t-Tests\n\n\n\nNúmero de observaciones para cada grupo:\n\ndf[\"sex\"].value_counts()\n\nsex\nmale      676\nfemale    662\nName: count, dtype: int64\n\n\nRealizamos un test de normalidad para cada grupo.\n\nmen = df[df[\"sex\"] == \"male\"]\nwomen = df[df[\"sex\"] == \"female\"]\n\n\ndef shapiro_test(data, alpha=0.05):\n    \"\"\"\n    Perform the Shapiro-Wilk test to check the normality of the data.\n\n    Parameters:\n    data (array-like): The data to analyze.\n    alpha (float): Significance level.\n\n    Returns:\n    str: The test result.\n    \"\"\"\n    statistic, p_value = stats.shapiro(data)\n\n    if p_value &lt; alpha:\n        return \"We reject the null hypothesis. The data does not follow a normal distribution.\"\n    else:\n        return \"We fail to reject the null hypothesis. The data can be considered normally distributed.\"\n\n\n# Run the Shapiro-Wilk test\nresult_men = shapiro_test(men[\"charges\"])\nresult_women = shapiro_test(women[\"charges\"])\n\nprint(\"For men:\", result_men)\nprint(\"For women:\", result_women)\n\nFor men: We reject the null hypothesis. The data does not follow a normal distribution.\nFor women: We reject the null hypothesis. The data does not follow a normal distribution.\n\n\nEstudiemos la relación entre las varianzas de cada grupo.\n\n# Perform the Levene's Test\nstatistic, p_value = stats.levene(men[\"bmi\"], women[\"bmi\"])\n\n# Significance level\nalpha = 0.05\n\n# Check for significance\nif p_value &lt; alpha:\n    print(\"We reject the null hypothesis. The variances are not similar.\")\nelse:\n    print(\"We fail to reject the null hypothesis. The variances are similar.\")\n\nWe fail to reject the null hypothesis. The variances are similar.\n\n\nVolvamos sobre los requisitos del test:\n\n¿Tenemos una sample representativa de la population? Suponemos que sí.\n¿Los datos son continuos? Sí.\n¿Las muestras siguen una distribución normal o hay más de 15 observaciones?\n\nNo, las muestras no siguen una distribución normal. Pero hay más de 15 observaciones en cada grupo, gracias al teorema central del límite podemos renunciar al supuesto de normalidad.\n\n¿Los grupos son independientes? Sí.\n¿Las varianzas son iguales (o al menos similares)? Sí.\n\nSe cumplen los requisitos para realizar un 2-Sample t-Test.\nPara realizar el test vamos a usar scipy.stats.ttest_ind.\n\n# Realiza el test t de Welch (equal_var=False)\nstatistic, p_value = stats.ttest_ind(men[\"bmi\"], women[\"bmi\"])\n\n# Check for significance\nalpha = 0.05  # Significance level\nif p_value &lt; alpha:\n    print(\n        \"We reject the null hypothesis. There are significant differences between the groups.\"\n    )\nelse:\n    print(\n        \"We fail to reject the null hypothesis. There are no significant differences between the groups.\"\n    )\n\nWe fail to reject the null hypothesis. There are no significant differences between the groups.\n\n\n\n\n\nIdeas destacadas\nNo tenemos evidencia suficiente para rechazar la hipótesis nula. No podemos afirmar que exista una diferencia en el BMI medio de hombres y mujeres.\n\n\n\n4.3 ¿La proporción de fumadores es diferente según la región?\n\n# Calculate the proportion of smokers by region\nsmokers_by_region = (\n    df.groupby(\"region\")[\"smoker\"]\n    .value_counts(normalize=True)\n    .rename(\"proportion\")\n    .reset_index()\n)\n\n# Create a bar chart to show the proportion of smokers by region\nfig = px.bar(\n    smokers_by_region,\n    x=\"region\",\n    y=\"proportion\",\n    color=\"smoker\",\n    title=\"Proportion of Smokers by Region\",\n    labels={\n        \"proportion\": \"Proportion of Policyholders\",\n        \"smoker\": \"Smoker\",\n        \"region\": \"Region\",\n    },\n    category_orders={\"smoker\": [\"yes\", \"no\"]},\n)  # Order the legend\n\n# Adjust the text to display the proportions as percentages and ensure it's inside the bar for better readability\nfig.update_traces(texttemplate=\"%{y:.1%}\", textposition=\"inside\")\n\nfig.show()\n\n\n                                                \n\n\nLa máxima diferencia entre regiones es de un 7.2%. ¿Es suficiente para afirmar que la proporción es diferente según la región? Vamos a comprobarlo con un test de hipótesis.\n\n1. Seleccionamos la hipótesis y el nivel de significancia\nH0: No existe una diferencia en la proporción de fumadores según la región. Es decir, las variables son independientes.\nHa: Existe una diferencia en la proporción de fumadores según la región\nalpha = 0.05\n\n\n2. Identificamos el tipo de test\nQueremos determinar si existe una relación estadísticamente significativa entre dos variables categóricas. Por lo que un Chi-Square Test parece lo adecuado.\n\n2.1 Requisitos del test\n\nLas variables son categóricas.\nLas observaciones son independientes.\nCada celda de la tabla de contingencia tiene un valor esperado de al menos 5.\n\nExaminemos nuestra sample para ver si podemos aplicar el test.\n\n\n\n\n\n\nLibro de refencia\n\n\n\n\n\nBook: Hypothesis Testing An Intuitive Guide For Making Data Driven Decisions\nPage: 315\nSection: Chi-Square Tests of Independence\n\n\n\nCreamos la tabla de contingencia.\n\ncontingency_table = pd.crosstab(df[\"region\"], df[\"smoker\"])\ncontingency_table\n\n\n\n\n\n\n\nsmoker\nno\nyes\n\n\nregion\n\n\n\n\n\n\nnortheast\n257\n67\n\n\nnorthwest\n267\n58\n\n\nsoutheast\n273\n91\n\n\nsouthwest\n267\n58\n\n\n\n\n\n\n\nVolvamos sobre los requisitos del test:\n\n¿Las variables son categóricas? Sí.\n¿Las observaciones son independientes? Sí.\n¿Cada celda de la tabla de contingencia tiene un valor esperado de al menos 5? Sí.\n\nSe cumplen los requisitos para realizar un Chi-Square Test.\nPara realizar el test utilizaremos stats.chi2_contingency.\n\n# Perform the Chi-square independence test\nc, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n\nprint(\"P-value:\", p_value)\n\n# Check for significance\nalpha = 0.05  # Significance level\nif p_value &lt; alpha:\n    print(\n        \"We reject the null hypothesis. There is a relationship between the variables.\"\n    )\nelse:\n    print(\"We fail to reject the null hypothesis\")\n\nP-value: 0.06171954839170541\nWe fail to reject the null hypothesis\n\n\n\n\n\nIdeas destacadas\nFallamos al rechazar la hipótesis nula. No podemos concluir que no existe relación entre fumadores y la región.\n\n\n\n4.4 ¿El promedio de BMI en las mujeres es diferente según el número de hijos que tengan?\nEn este caso vamos a trabajar con los datos de las mujeres que tienen entre 0 y 2 hijos.\n\nwomen_few_children_df = df.loc[\n    (df[\"sex\"] == \"female\") & (df[\"children\"].isin([0, 1, 2]))\n]\n\nPrimero hagamos una exploración visual.\n\nfig = px.violin(\n    women_few_children_df,\n    x=\"bmi\",\n    color=\"children\",\n    box=True,\n    points=\"outliers\",\n)\n\nfig.update_layout(\n    title_text=\"BMI violin plot by children\",\n)\n\nfig.show()\n\n\n                                                \n\n\nParece que no existen grandes diferencias. ¿Pero es estadísticamente significativo? Vamos a comprobarlo con un test de hipótesis.\n\n1. Seleccionamos la hipótesis y el nivel de significancia\nH0: Los tres grupos tienen el mismo promedio de BMI\nHa: Al menos uno de los grupos tiene un promedio de BMI diferente\nalpha = 0.05\n\n\n2. Identificamos el tipo de test\nDeseamos comparar las medias de tres grupos de nuestra muestra. Por lo que un One-Way ANOVA parece lo adecuado. Lo primero es ver si se cumplen los requisitos del test.\n\n2.1 Requisitos del test\n\nLa variable dependiente es continua.\nLa variable independiente es categórica.\nLas muestras siguen una distribución normal o hay más de 20 observaciones.\nLos grupos son independientes.\nLas varianzas son similares.\n\nExaminemos nuestra sample para ver si podemos aplicar el test.\n\n\n\n\n\n\nLibro de refencia\n\n\n\n\n\nBook: Hypothesis Testing An Intuitive Guide For Making Data Driven Decisions\nPage: 197\nSection: One-Way ANOVA\n\n\n\nNúmero de observaciones para cada grupo:\n\nwomen_few_children_df[\"children\"].value_counts()\n\nchildren\n0    289\n1    158\n2    119\nName: count, dtype: int64\n\n\n\n# Run the Shapiro-Wilk test\nresult_zero = shapiro_test(df[df[\"children\"] == 0][\"bmi\"])\nresult_one = shapiro_test(df[df[\"children\"] == 1][\"bmi\"])\nresult_two = shapiro_test(df[df[\"children\"] == 2][\"bmi\"])\n\nprint(\"For zero child:\", result_zero)\nprint(\"For one child:\", result_one)\nprint(\"For two children:\", result_two)\n\nFor zero child: We reject the null hypothesis. The data does not follow a normal distribution.\nFor one child: We reject the null hypothesis. The data does not follow a normal distribution.\nFor two children: We fail to reject the null hypothesis. The data can be considered normally distributed.\n\n\n\n# Perform the Levene's Test\nstatistic, p_value = stats.levene(\n    df[df[\"children\"] == 0][\"bmi\"],\n    df[df[\"children\"] == 1][\"bmi\"],\n    df[df[\"children\"] == 2][\"bmi\"],\n)\n\n# Significance level\nalpha = 0.05\n\n# Check for significance\nif p_value &lt; alpha:\n    print(\"We reject the null hypothesis. The variances are not similar.\")\nelse:\n    print(\"We fail to reject the null hypothesis. The variances are similar.\")\n\nWe fail to reject the null hypothesis. The variances are similar.\n\n\nVolvamos sobre los requisitos del test:\n\n¿La variable dependiente es continua? Sí.\n¿La variable independiente es categórica? Sí.\n¿Las muestras siguen una distribución normal o hay más de 20 observaciones\n\nNo, las muestras no siguen una distribución normal. Pero hay más de 20 observaciones en cada grupo, gracias al teorema central del límite podemos renunciar al supuesto de normalidad.\n\n¿Los grupos son independientes? Sí.\n¿Las varianzas son similares? Sí.\n\nSe cumplen los requisitos para realizar un One-Way ANOVA.\nPara realizar el test utilizaremos scipy.stats.f_oneway.\n\nf_statistic, p_value = stats.f_oneway(\n    df[df[\"children\"] == 0][\"bmi\"],\n    df[df[\"children\"] == 1][\"bmi\"],\n    df[df[\"children\"] == 2][\"bmi\"],\n)\n\n# Check for significance\nalpha = 0.05  # Significance level\nif p_value &lt; alpha:\n    print(\n        \"We reject the null hypothesis. There are significant differences between the groups.\"\n    )\nelse:\n    print(\n        \"We fail to reject the null hypothesis. There are no significant differences between the groups.\"\n    )\n\nWe fail to reject the null hypothesis. There are no significant differences between the groups.\n\n\n\n\n\nIdeas destacadas\nFallamos al rechazar la hipótesis nula. No hay diferencias en el BMI en función del número de hijos.\n\n\n\n4.5 ¿Es diferente la proporción de fumadores en ambos sexos?\nPrimero hagamos una exploración visual.\n\n# Calculate the proportion of smokers by sex\nsmokers_by_sex = (\n    df.groupby(\"sex\")[\"smoker\"]\n    .value_counts(normalize=True)\n    .rename(\"proportion\")\n    .reset_index()\n)\n\n# Create a bar chart to show the proportion of smokers by sex\nfig = px.bar(\n    smokers_by_sex,\n    x=\"sex\",\n    y=\"proportion\",\n    color=\"smoker\",\n    title=\"Proportion of Smokers by sex\",\n    labels={\n        \"proportion\": \"Proportion of Policyholders\",\n        \"smoker\": \"Smoker\",\n        \"sex\": \"sex\",\n    },\n    category_orders={\"smoker\": [\"yes\", \"no\"]},\n)  # Order the legend\n\n# Adjust the text to display the proportions as percentages and ensure it's inside the bar for better readability\nfig.update_traces(texttemplate=\"%{y:.1%}\", textposition=\"inside\")\n\nfig.show()\n\n\n                                                \n\n\nParece que existen diferencias. ¿Pero son estadísticamente significativas? Vamos a comprobarlo con un test de hipótesis.\n\n1. Seleccionamos la hipótesis y el nivel de significancia\nH0: La proporcion de fumadores es igual en ambos sexos.\nHa: La proporcion de fumadores es diferente.\nalpha = 0.05\n\n\n2. Identificamos el tipo de test\nDeseamos comparar la proporción en dos variables binarias. Por lo que un Two-Sample Proportion Test parece lo adecuado. Lo primero es ver si se cumplen los requisitos del test.\n\n2.1 Requisitos del test\n\nVariables binarias.\nGrupos independientes.\nCada muestra es independiente.\nLas proporciones se mantienen constantes en el tiempo.\n\nExaminemos nuestra sample para ver si podemos aplicar el test.\n\n\n\n\n\n\nLibro de refencia\n\n\n\n\n\nBook: Hypothesis Testing An Intuitive Guide For Making Data Driven Decisions\nPage: 283\nSection: Two-Sample Proportion Test\n\n\n\nVolvamos sobre los requisitos del test:\n\n¿Variables binarias? Sí.\n¿Grupos independientes? Sí.\n¿Cada muestra es independiente? Sí.\n¿Las proporciones se mantienen constantes en el tiempo? Sí.\n\nSe cumplen los requisitos para realizar un Two-Sample Proportion Test.\nPara realizar el test vamos a usar statsmodels.stats.proportion.proportions_ztest.\n\ncontingency_table = pd.crosstab(df[\"sex\"], df[\"smoker\"])\ncontingency_table\n\n\n\n\n\n\n\nsmoker\nno\nyes\n\n\nsex\n\n\n\n\n\n\nfemale\n547\n115\n\n\nmale\n517\n159\n\n\n\n\n\n\n\n\n# Create two groups based on \"sex\" and \"smoker\"\ngroup1 = df[(df[\"sex\"] == \"female\") & (df[\"smoker\"] == \"yes\")]\ngroup2 = df[(df[\"sex\"] == \"female\") & (df[\"smoker\"] == \"no\")]\n\n# Count the number of observations in each group\ncount1 = len(group1)\ncount2 = len(group2)\n\n# Count the number of smokers (successes) in each group\nsuccess1 = len(group1[group1[\"smoker\"] == \"yes\"])\nsuccess2 = len(group2[group2[\"smoker\"] == \"yes\"])\n\n# Perform the Two-Sample Proportions Test\nstat, p_value = sm.stats.proportions_ztest([success1, success2], [count1, count2])\n\n# Check for significance\nalpha = 0.05\n\nif p_value &lt; alpha:\n    print(\n        \"We reject the null hypothesis. There are significant differences in proportions between sexes.\"\n    )\nelse:\n    print(\n        \"We fail to reject the null hypothesis. There are no significant differences in proportions between sexes.\"\n    )\n\nWe reject the null hypothesis. There are significant differences in proportions between sexes.\n\n\n\n\n\nIdeas destacadas\nRechazamos la hipótesis nula. Existen diferencias en a proporción de fumadores entre ambos sexos."
  },
  {
    "objectID": "posts/2023-11-14-health_insurance_hypothesis_testing/health-Insurance_hypothesis_testing.html#recapitulación-y-reflexiones",
    "href": "posts/2023-11-14-health_insurance_hypothesis_testing/health-Insurance_hypothesis_testing.html#recapitulación-y-reflexiones",
    "title": "Test de Hipótesis: explorando correlaciones en datos de seguros de salud",
    "section": "5. Recapitulación y reflexiones",
    "text": "5. Recapitulación y reflexiones\nRepasemos nuestras preguntas iniciales y lo que hemos averiguado al respecto:\n\n¿Los fumadores generan más gasto a la aseguradora que los no fumadores?\n\nSí, con una evidencia muy alta.\n\n¿Las mujeres tienen un BMI diferente a los hombres?\n\nNo, no podemos afirmar que exista una diferencia en el BMI medio de hombres y mujeres.\n\n¿La proporción de fumadores es diferente según la región?\n\nNo, no podemos concluir que no existe relación entre fumadores y región.\n\n¿El promedio de BMI en las mujeres es diferente según el número de hijos que tengan?\n\nNo, no hay diferencias en el BMI en función del número de hijos.\n\n¿Es diferente la proporción de fumadores en ambos sexos?\n\nSí, los hombres tienen una mayor proporción de fumadores."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Lorem ipsum dolor sit amet consectetur adipisicing elit. Maxime mollitia, molestiae quas vel sint commodi repudiandae consequuntur voluptatum laborum numquam blanditiis harum quisquam eius sed odit fugiat iusto fuga praesentium optio, eaque rerum! Provident similique accusantium nemo autem. Veritatis obcaecati tenetur iure eius earum ut molestias architecto voluptate aliquam nihil, eveniet aliquid culpa officia aut! Impedit sit sunt quaerat, odit, tenetur error, harum nesciunt ipsum debitis quas aliquid. Reprehenderit, quia. Quo neque error repudiandae fuga? Ipsa laudantium molestias eos sapiente officiis modi at sunt excepturi expedita sint? Sed quibusdam recusandae alias error harum maxime adipisci amet laborum. Perspiciatis minima nesciunt dolorem! Officiis iure rerum voluptates a cumque velit quibusdam sed amet tempora. Sit laborum ab, eius fugit doloribus tenetur fugiat, temporibus enim commodi iusto libero magni deleniti quod quam consequuntur! Commodi minima excepturi repudiandae velit hic maxime doloremque. Quaerat provident commodi consectetur veniam similique ad earum omnis ipsum saepe, voluptas, hic voluptates pariatur est explicabo fugiat, dolorum eligendi quam cupiditate excepturi mollitia maiores labore suscipit quas? Nulla, placeat. Voluptatem quaerat non architecto ab laudantium modi minima sunt esse temporibus sint culpa, recusandae aliquam numquam totam ratione voluptas quod exercitationem fuga. Possimus quis earum veniam quasi aliquam eligendi, placeat qui corporis!"
  },
  {
    "objectID": "posts/2023-10-16-bestsellers_hypothesis_testing/bestsellers_with_categories_hypothesis_testing.html",
    "href": "posts/2023-10-16-bestsellers_hypothesis_testing/bestsellers_with_categories_hypothesis_testing.html",
    "title": "Test de Hipótesis: Análisis de los 50 libros más vendidos en amazon (2009 - 2019)",
    "section": "",
    "text": "From Kaggle: Amazon Top 50 Bestselling Books 2009 - 2019\n\n\nDataset sobre los 50 libros más vendidos de Amazon de 2009 a 2019. Contiene 550 libros, los datos se han clasificado en ficción y no-ficción utilizando Goodreads.\n\n\n\nVamos a responder a las siguientes preguntas con validez estadística:\n\n¿Los géneros difieren en “User Rating”?\n¿Los géneros difieren en número de “Reviews”?\n¿Los géneros difieren en términos de “Price”?\n\n\n\n\n\n\n\nMatemáticas y código: en Inglés\n\n\n\n\n\nAunque el texto principal está en español, los términos matemáticos y el código están en inglés.\nEsta práctica sigue el estándar internacional y ayuda a familiarizarse con el lenguaje técnico más utilizado en el campo de la ciencia de datos."
  },
  {
    "objectID": "posts/2023-10-16-bestsellers_hypothesis_testing/bestsellers_with_categories_hypothesis_testing.html#a.-entendimiento-del-negocio",
    "href": "posts/2023-10-16-bestsellers_hypothesis_testing/bestsellers_with_categories_hypothesis_testing.html#a.-entendimiento-del-negocio",
    "title": "Test de Hipótesis: Análisis de los 50 libros más vendidos en amazon (2009 - 2019)",
    "section": "",
    "text": "From Kaggle: Amazon Top 50 Bestselling Books 2009 - 2019\n\n\nDataset sobre los 50 libros más vendidos de Amazon de 2009 a 2019. Contiene 550 libros, los datos se han clasificado en ficción y no-ficción utilizando Goodreads.\n\n\n\nVamos a responder a las siguientes preguntas con validez estadística:\n\n¿Los géneros difieren en “User Rating”?\n¿Los géneros difieren en número de “Reviews”?\n¿Los géneros difieren en términos de “Price”?\n\n\n\n\n\n\n\nMatemáticas y código: en Inglés\n\n\n\n\n\nAunque el texto principal está en español, los términos matemáticos y el código están en inglés.\nEsta práctica sigue el estándar internacional y ayuda a familiarizarse con el lenguaje técnico más utilizado en el campo de la ciencia de datos."
  },
  {
    "objectID": "posts/2023-10-16-bestsellers_hypothesis_testing/bestsellers_with_categories_hypothesis_testing.html#b.-descripción-de-datos",
    "href": "posts/2023-10-16-bestsellers_hypothesis_testing/bestsellers_with_categories_hypothesis_testing.html#b.-descripción-de-datos",
    "title": "Test de Hipótesis: Análisis de los 50 libros más vendidos en amazon (2009 - 2019)",
    "section": "1b. Descripción de datos",
    "text": "1b. Descripción de datos\nPara empezar, importamos las librerías que vamos a utilizar:\n\nPandas: Pandas es una biblioteca esencial en la ciencia de datos que proporciona estructuras de datos flexibles y eficientes, como DataFrames, para el análisis y manipulación de datos tabulares. Es ampliamente utilizada para limpiar, transformar y analizar datos, lo que la convierte en una herramienta fundamental para la preparación de datos en proyectos de ciencia de datos.\nScipy: Scipy es una biblioteca que se construye sobre NumPy y ofrece una amplia variedad de módulos y funciones especializadas para aplicaciones científicas y matemáticas. Incluye herramientas para estadísticas, optimización, álgebra lineal y procesamiento de señales, lo que la hace esencial en la investigación y el análisis de datos en ciencia de datos.\nFuzzywuzzy: Fuzzywuzzy es una biblioteca que se utiliza en la ciencia de datos para comparar cadenas de texto difusas o parcialmente coincidentes. Es útil en la limpieza y normalización de datos de texto, así como en la identificación de similitudes entre strings, lo que es valioso en tareas como la duplicación de registros o la coincidencia de nombres en bases de datos.\nPlotly Express: Plotly Express es una biblioteca de visualización de datos que simplifica la creación de gráficos interactivos y visuales. Es especialmente útil en la exploración de datos y la comunicación de resultados en ciencia de datos, permitiendo a los científicos de datos crear visualizaciones informativas y atractivas con facilidad.\n\n\n\nCode\n# Import libraries\nimport pandas as pd\nfrom scipy import stats\nfrom fuzzywuzzy import fuzz\n\n# Import plotly and customize\nimport plotly.io as pio\nimport plotly.express as px\n\n# Set a global template, e.g. \"plotly_dark\"\n# Customize the color scheme of the chosen template, e.g. \"Set2\"\npio.templates.default = \"plotly\"\npio.templates[\"plotly\"].layout.colorway = px.colors.qualitative.Set2\n\n\nCargamos el dataset y describimos brevemente sus características.\n\ndf = pd.read_csv(\"bestsellers with categories.csv\")\ndf.head(5)\n\n\n\n\n\n\n\n\nName\nAuthor\nUser Rating\nReviews\nPrice\nYear\nGenre\n\n\n\n\n0\n10-Day Green Smoothie Cleanse\nJJ Smith\n4.7\n17350\n8\n2016\nNon Fiction\n\n\n1\n11/22/63: A Novel\nStephen King\n4.6\n2052\n22\n2011\nFiction\n\n\n2\n12 Rules for Life: An Antidote to Chaos\nJordan B. Peterson\n4.7\n18979\n15\n2018\nNon Fiction\n\n\n3\n1984 (Signet Classics)\nGeorge Orwell\n4.7\n21424\n6\n2017\nFiction\n\n\n4\n5,000 Awesome Facts (About Everything!) (Natio...\nNational Geographic Kids\n4.8\n7665\n12\n2019\nNon Fiction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCampo\nTipo de Dato\nDescripción\nEjemplo\n\n\n\n\nName\nTexto\nTítulo del libro.\n11/22/63: A Novel\n\n\nAuthor\nTexto\nAutor del libro.\nStephen King\n\n\nUser Rating\nNumérico entre 0 y 5 (con un decimal)\nCalificación promedio otorgada por usuarios.\n4.6\n\n\nReviews\nNumérico (entero)\nCantidad de reseñas del libro.\n2052\n\n\nPrice\nNumérico (entero)\nPrecio de venta del libro.\n22\n\n\nYear\nAño (número entero)\nAño de inclusión en la lista de más vendidos.\n2011\n\n\nGenre\nCategórico\nGénero del libro, ficción o no ficción.\nFicción\n\n\n\nEstos son los tipos de datos que se identifican en primer momento. Como veremos más adelante, estas designaciones pueden ser problemáticas según los valores que contenga el dataset y los insights que queramos obtener."
  },
  {
    "objectID": "posts/2023-10-16-bestsellers_hypothesis_testing/bestsellers_with_categories_hypothesis_testing.html#a.-preparación-de-los-datos",
    "href": "posts/2023-10-16-bestsellers_hypothesis_testing/bestsellers_with_categories_hypothesis_testing.html#a.-preparación-de-los-datos",
    "title": "Test de Hipótesis: Análisis de los 50 libros más vendidos en amazon (2009 - 2019)",
    "section": "2a. Preparación de los datos",
    "text": "2a. Preparación de los datos\n\n2a.1 Typecasting\nComprobamos el tipo de datos de las columnas y los modificamos conforme nuestra descripción inicial.\n\ndf.dtypes\n\nName            object\nAuthor          object\nUser Rating    float64\nReviews          int64\nPrice            int64\nYear             int64\nGenre           object\ndtype: object\n\n\n\ncategorical_columns = [\"Genre\", \"Name\", \"Author\"]\ndf[categorical_columns] = df[categorical_columns].astype(\"category\")\n\n\ndf.dtypes\n\nName           category\nAuthor         category\nUser Rating     float64\nReviews           int64\nPrice             int64\nYear              int64\nGenre          category\ndtype: object\n\n\n\n\n2a.2 Manejo de duplicados\n\nSimpler approach\n¿Cuantas filas son exactamente iguales?\n\ndf.duplicated().sum()\n\n0\n\n\nEs normal no encontrar duplicados en este caso, ya que hay libros que se repiten, pero es imposible que coincidan en “Year”. No es un error en sí mismo. Sin embargo, hay un problema. Las “Reviews” deberían ser las que el libro tenía en el año en el que fue uno de los más vendidos. En lugar de eso, son las del último año. Esto podría deberse a que el autor del dataset no tuvo acceso al historial de las reseñas.\n\n\nRepeated Bestsellers\n\ndf[df.duplicated(\"Name\")]\n\n\n\n\n\n\n\n\nName\nAuthor\nUser Rating\nReviews\nPrice\nYear\nGenre\n\n\n\n\n10\nA Man Called Ove: A Novel\nFredrik Backman\n4.6\n23848\n8\n2017\nFiction\n\n\n21\nAll the Light We Cannot See\nAnthony Doerr\n4.6\n36348\n14\n2015\nFiction\n\n\n33\nBecoming\nMichelle Obama\n4.8\n61133\n11\n2019\nNon Fiction\n\n\n36\nBetween the World and Me\nTa-Nehisi Coates\n4.7\n10070\n13\n2016\nNon Fiction\n\n\n41\nBrown Bear, Brown Bear, What Do You See?\nBill Martin Jr.\n4.9\n14344\n5\n2019\nFiction\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n543\nWonder\nR. J. Palacio\n4.8\n21625\n9\n2016\nFiction\n\n\n544\nWonder\nR. J. Palacio\n4.8\n21625\n9\n2017\nFiction\n\n\n547\nYou Are a Badass: How to Stop Doubting Your Gr...\nJen Sincero\n4.7\n14331\n8\n2017\nNon Fiction\n\n\n548\nYou Are a Badass: How to Stop Doubting Your Gr...\nJen Sincero\n4.7\n14331\n8\n2018\nNon Fiction\n\n\n549\nYou Are a Badass: How to Stop Doubting Your Gr...\nJen Sincero\n4.7\n14331\n8\n2019\nNon Fiction\n\n\n\n\n199 rows × 7 columns\n\n\n\nHay 199 libros que han sido bestsellers durante más de un año. Es importante tener en cuenta esto para el análisis. Vamos a utilizar únicamente la muestra más reciente de cada libro.\n\nbestsellers = df.drop_duplicates(subset=\"Name\", keep=\"last\")\n\n\n\nChecking for authors or titles with different spellings\nComprobamos si hay autores o títulos iguales pero escritos diferente.\n\n# Function to find similar author names\ndef find_similar(df, column):\n    authors = df[column].unique()\n    duplicates = []\n\n    for i, author1 in enumerate(authors):\n        for author2 in authors[i + 1 :]:\n            ratio = fuzz.ratio(author1, author2)\n            if ratio &gt; 90:  # You can adjust this threshold according to your criteria\n                duplicates.append((author1, author2))\n\n    return duplicates\n\n\n# Find similar author names\nduplicates = find_similar(bestsellers, \"Author\")\n\nprint(\"Author names that refer to the same author but are written differently:\")\nfor dup in duplicates:\n    print(dup)\n\n# Find similar title names\nduplicates = find_similar(bestsellers, \"Name\")\n\nprint(\"Title names that refer to the same title but are written differently:\")\nfor dup in duplicates:\n    print(dup)\n\nAuthor names that refer to the same author but are written differently:\n('George R. R. Martin', 'George R.R. Martin')\n('J.K. Rowling', 'J. K. Rowling')\nTitle names that refer to the same title but are written differently:\n('The 5 Love Languages: The Secret to Love That Lasts', 'The 5 Love Languages: The Secret to Love that Lasts')\n('The Girl Who Played with Fire (Millennium Series)', 'The Girl Who Played with Fire (Millennium)')\n\n\n\n# Replace the names of the Authors with the correct ones\nbestsellers = bestsellers.replace(\"George R. R. Martin\", \"George R.R. Martin\")\nbestsellers = bestsellers.replace(\"J. K. Rowling\", \"J.K. Rowling\")\n\n# Replace the names of the Authors with the correct ones\nbestsellers = bestsellers.replace(\n    \"The 5 Love Languages: The Secret to Love That Lasts\",\n    \"The 5 Love Languages: The Secret to Love that Lasts\",\n)\nbestsellers = bestsellers.replace(\n    \"The Girl Who Played with Fire (Millennium Series)\",\n    \"The Girl Who Played with Fire (Millennium)\",\n)\n\n\nbestsellers\n\n\n\n\n\n\n\n\nName\nAuthor\nUser Rating\nReviews\nPrice\nYear\nGenre\n\n\n\n\n0\n10-Day Green Smoothie Cleanse\nJJ Smith\n4.7\n17350\n8\n2016\nNon Fiction\n\n\n1\n11/22/63: A Novel\nStephen King\n4.6\n2052\n22\n2011\nFiction\n\n\n2\n12 Rules for Life: An Antidote to Chaos\nJordan B. Peterson\n4.7\n18979\n15\n2018\nNon Fiction\n\n\n3\n1984 (Signet Classics)\nGeorge Orwell\n4.7\n21424\n6\n2017\nFiction\n\n\n4\n5,000 Awesome Facts (About Everything!) (Natio...\nNational Geographic Kids\n4.8\n7665\n12\n2019\nNon Fiction\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n538\nWinter of the World: Book Two of the Century T...\nKen Follett\n4.5\n10760\n15\n2012\nFiction\n\n\n539\nWomen Food and God: An Unexpected Path to Almo...\nGeneen Roth\n4.2\n1302\n11\n2010\nNon Fiction\n\n\n544\nWonder\nR. J. Palacio\n4.8\n21625\n9\n2017\nFiction\n\n\n545\nWrecking Ball (Diary of a Wimpy Kid Book 14)\nJeff Kinney\n4.9\n9413\n8\n2019\nFiction\n\n\n549\nYou Are a Badass: How to Stop Doubting Your Gr...\nJen Sincero\n4.7\n14331\n8\n2019\nNon Fiction\n\n\n\n\n351 rows × 7 columns\n\n\n\n\n\nIdeas destacadas\n\nHemos corregido las variaciones ortográficas de los títulos y autores.\nHemos seleccionado las muestras más recientes de cada libro.\n\nCon esto, se ha reducido el tamaño del dataset de 550 a 351 muestras.\n\n\n\n2a.3 Análisis de valores atípicos\n\nselected_columns = [\"Reviews\", \"Price\"]\n\nfor column in selected_columns:\n    fig = px.box(df, x=column, title=f\"Boxplot of {column}\")\n    fig.update_layout(title_x=0.5)\n    fig.show()\n\n\n                                                \n\n\n\n                                                \n\n\n\nIdeas destacadas\nSe pueden hacer tres cosas con los outliers, siguiendo la mnemotecnia 3R: rectificar, retener o remover. En este caso, podemos estar ante valores atípicos genuinos, por tanto vamos a retenerlos por el momento.\n\n\n\n2a.4 Variables con varianza cercana a cero\nSi la variance es cero, la variable no aporta información para el análisis estadístico o el modelado, por lo tanto, puede ser eliminada.\nSi la variance es casi cero, también es una variable candidata a ser eliminada, ya que aporta poca información y puede generar ruido en el análisis.\nEs importante tener en cuenta que el umbral para considerar una variable como “Near Zero Variance” puede variar según el contexto y el problema específico que se esté abordando.\n\nselected_columns = [\"User Rating\", \"Reviews\", \"Price\"]\n\n# Calculate the variance of each column\nvariances = bestsellers[selected_columns].var()\n\n# Define a threshold for variance\nthreshold = 0.1\n\n# Identify columns with near-zero or very small variance\nzero_variance_cols = variances[variances &lt;= threshold].index\n\nprint(\"Columns with Zero & Near Zero Variance:\")\nprint(zero_variance_cols)\n\nColumns with Zero & Near Zero Variance:\nIndex(['User Rating'], dtype='object')\n\n\nLa variable “User Rating” parece estar por debajo del umbral que hemos escogido arbitrariamente. Vamos a representarla gráficamente para comprender mejor la situación.\n\ncolumn = \"User Rating\"\nfig = px.violin(df, x=column, title=f\"Violin plot of {column}\", points=\"all\")\nfig.update_layout(title_x=0.5, xaxis=dict(range=[0, 5.5]))\nfig.show()\n\n\n                                                \n\n\n\nIdeas destacadas\nAl trabajar con un dataset de bestsellers, la mayoría de los “User Rating” están entre 4 y 5 estrellas, mientras que inicialmente se esperaba que las muestras estuvieran repartidas entre 0 y 5.\nEs importante tener esto presente en posteriores análisis, ya que casi podría considerarse una variable categórica en lugar de continua.\n\n\n\n2a.5 Valores ausentes o faltantes\n\nbestsellers.isnull().sum()\n\nName           0\nAuthor         0\nUser Rating    0\nReviews        0\nPrice          0\nYear           0\nGenre          0\ndtype: int64\n\n\n\nIdeas destacadas\n\nNo hay missing values"
  },
  {
    "objectID": "posts/2023-10-16-bestsellers_hypothesis_testing/bestsellers_with_categories_hypothesis_testing.html#pruebas-de-hipótesis",
    "href": "posts/2023-10-16-bestsellers_hypothesis_testing/bestsellers_with_categories_hypothesis_testing.html#pruebas-de-hipótesis",
    "title": "Test de Hipótesis: Análisis de los 50 libros más vendidos en amazon (2009 - 2019)",
    "section": "4. Pruebas de hipótesis",
    "text": "4. Pruebas de hipótesis\nVamos a responder a las siguientes preguntas con validez estadística:\n\n¿Los géneros difieren en “User Rating”?\n¿Los géneros difieren en número de “Reviews”?\n¿Los géneros difieren en términos de “Price”?\n\n\n4.1 ¿Los géneros difieren en “User Rating”?\nPrimero hagamos una exploración visual.\n\nfig = px.violin(\n    bestsellers,\n    x=\"User Rating\",\n    color=\"Genre\",\n    box=True,\n    points=\"outliers\",\n)\n\nfig.update_layout(\n    title_text=\"User Rating violin plot by Genre\",\n)\n\nfig.show()\n\n\n                                                \n\n\nParece que existen diferencias. ¿Pero son estadísticamente significativas? Vamos a comprobarlo con un test de hipótesis.\n\n1. Seleccionamos la hipótesis y el nivel de significancia\nH0: El “User Rating” de los libros de ficción es igual que el de los libros de no ficción.\nHa: El “User Rating” de los libros de ficción es diferente al de los libros de no ficción.\nalpha = 0.05\n\n\n2. Identificamos el tipo de test\nDeseamos comparar las medias de dos grupos de nuestra sample. Por lo tanto, un 2-Sample t-Test parece ser adecuado. Lo primero es verificar si se cumplen los requisitos del test.\n\n2.1 Requisitos del test\n\nTenemos una sample representativa de la population.\nLos datos son continuos.\nLas muestras siguen una distribución normal o hay más de 15 observaciones.\nLos grupos son independientes.\nLas varianzas son iguales (o al menos similares).\n\nExaminemos nuestra sample para ver si podemos aplicar el test.\n\n\n\n\n\n\nLibro de refencia\n\n\n\n\n\nBook: Hypothesis Testing An Intuitive Guide For Making Data Driven Decisions\nPage: 48\nSection: 2-Sample t-Tests\n\n\n\nNúmero de observaciones para cada grupo:\n\nbestsellers[\"Genre\"].value_counts()\n\nGenre\nNon Fiction    191\nFiction        160\nName: count, dtype: int64\n\n\nRealizamos un test de normalidad para cada grupo:\n\nfiction_bestsellers = bestsellers[bestsellers[\"Genre\"] == \"Fiction\"]\nnonfiction_bestsellers = bestsellers[bestsellers[\"Genre\"] == \"Non Fiction\"]\n\n\ndef shapiro_test(data, alpha=0.05):\n    \"\"\"\n    Perform the Shapiro-Wilk test to check the normality of the data.\n\n    Parameters:\n    data (array-like): The data to analyze.\n    alpha (float): Significance level.\n\n    Returns:\n    str: The test result.\n    \"\"\"\n    statistic, p_value = stats.shapiro(data)\n\n    if p_value &lt; alpha:\n        return \"We reject the null hypothesis. The data does not follow a normal distribution.\"\n    else:\n        return \"We fail to reject the null hypothesis. The data can be considered normally distributed.\"\n\n\nresult_fiction = shapiro_test(fiction_bestsellers[\"User Rating\"])\nresult_nonfiction = shapiro_test(nonfiction_bestsellers[\"User Rating\"])\n\nprint(\"For fiction bestsellers:\", result_fiction)\nprint(\"For non-fiction bestsellers:\", result_nonfiction)\n\nFor fiction bestsellers: We reject the null hypothesis. The data does not follow a normal distribution.\nFor non-fiction bestsellers: We reject the null hypothesis. The data does not follow a normal distribution.\n\n\nEstudiemos la relación entre las varianzas de cada grupo.\n\n# Perform the Levene's Test\nstatistic, p_value = stats.levene(\n    fiction_bestsellers[\"User Rating\"], nonfiction_bestsellers[\"User Rating\"]\n)\n\n# Significance level\nalpha = 0.05\n\n# Check for significance\nif p_value &lt; alpha:\n    print(\"We reject the null hypothesis. The variances are not similar.\")\nelse:\n    print(\"We fail to reject the null hypothesis. The variances are similar.\")\n\nWe reject the null hypothesis. The variances are not similar.\n\n\nVolvamos sobre los requisitos del test:\n\n¿Tenemos una sample representativa de la population? Suponemos que sí.\n¿Los datos son continuos?\n\nEn este contexto, los “User Rating” que van de 0 a 5 con un único decimal pueden considerarse como datos continuos. Sin embargo, observando las gráficas, vemos que la mayoría de los valores se encuentran en un intervalo muy pequeño (mayores a 4.0). Esto dificulta considerarlos como continuos.\n\n¿Las muestras siguen una distribución normal o hay más de 15 observaciones?\n\nNo, las muestras no siguen una distribución normal. Pero hay más de 15 observaciones en cada grupo, gracias al teorema central del límite podemos renunciar al supuesto de normalidad.\n\n¿Los grupos son independientes? Sí.\n¿Las varianzas son iguales (o al menos similares)?\n\nNo.\n\n\nDada la falta de continuidad en los datos, vamos a realizar un test no paramétrico Mann-Whitney.\nPara realizar el test utilizaremos scipy.stats.mannwhitneyu\n\n\n\n\n\n\nLibro de refencia\n\n\n\n\n\nBook:Hypothesis Testing An Intuitive Guide For Making Data Driven Decisions\nPage: 341\nSection: Analyzing Likert Scale Data\n\n\n\n\n# Perform the Mann-Whitney U test\nstatistic, p_value = stats.mannwhitneyu(\n    nonfiction_bestsellers[\"User Rating\"], fiction_bestsellers[\"User Rating\"]\n)\n\n# Print the results\nprint(\"p-value:\", p_value)\n\n# Check for significance\nalpha = 0.05  # Significance level\nif p_value &lt; alpha:\n    print(\n        \"We reject the null hypothesis. There are significant differences between the groups.\"\n    )\nelse:\n    print(\n        \"We fail to reject the null hypothesis. There are no significant differences between the groups.\"\n    )\n\np-value: 0.019226481868505015\nWe reject the null hypothesis. There are significant differences between the groups.\n\n\n\n\n\nIdeas destacadas\nPodemos afirmar que hay diferencias significativas en términos de “User Rating” entre los libros de ficción y los de no ficción.\n\n\n\n4.2 ¿Los géneros difieren en número de “Reviews”?\nPrimero hagamos una exploración visual.\n\nfig = px.violin(\n    bestsellers,\n    x=\"Reviews\",\n    color=\"Genre\",\n    box=True,\n    points=\"outliers\",\n)\n\nfig.update_layout(\n    title_text=\"Reviews violin plot by Genre\",\n)\n\nfig.show()\n\n\n                                                \n\n\nParece que existen difencias. ¿Pero son estadisticamente significativas? Vamos a comprobarlo con un test de hipótesis.\n\n1. Seleccionamos la hipotesis y el nivel de significancia\nH0: El número de ‘Reviews’ medio de los libros de ficción es igual que el de los libros de no ficción\nHa: El número de ‘Reviews’ medio de los libros de ficción es diferente que el de los libros de no ficción\nalpha = 0.05\n\n\n2. Identificamos el tipo de test\nDeseamos comparar las medias de dos grupos de nuestra sample. Por lo tanto, un 2-Sample t-Test parece ser adecuado. Lo primero es verificar si se cumplen los requisitos del test.\n\n2.1 Requisitos del test\n\nTenemos una sample representativa de la population.\nLos datos son continuos.\nLas muestras siguen una distribución normal o hay más de 15 observaciones.\nLos grupos son independientes.\nLas varianzas son iguales (o al menos similares).\n\nExaminemos nuestra sample para ver si podemos aplicar el test.\n\n\n\n\n\n\nLibro de refencia\n\n\n\n\n\nBook: Hypothesis Testing An Intuitive Guide For Making Data Driven Decisions\nPage: 48\nSection: 2-Sample t-Tests\n\n\n\nNúmero de observaciones para cada grupo:\n\nbestsellers[\"Genre\"].value_counts()\n\nGenre\nNon Fiction    191\nFiction        160\nName: count, dtype: int64\n\n\nRealizamos un test de normalidad para cada grupo:\n\nresult_fiction = shapiro_test(fiction_bestsellers[\"Reviews\"])\nresult_nonfiction = shapiro_test(nonfiction_bestsellers[\"Reviews\"])\n\nprint(\"Para bestsellers de ficción:\", result_fiction)\nprint(\"Para bestsellers de no ficción:\", result_nonfiction)\n\nPara bestsellers de ficción: We reject the null hypothesis. The data does not follow a normal distribution.\nPara bestsellers de no ficción: We reject the null hypothesis. The data does not follow a normal distribution.\n\n\nEstudiemos la relación entre las varianzas de cada grupo.\n\n# Perform the Levene's Test\nstatistic, p_value = stats.levene(\n    fiction_bestsellers[\"User Rating\"], nonfiction_bestsellers[\"User Rating\"]\n)\n\n# Significance level\nalpha = 0.05\n\n# Check for significance\nif p_value &lt; alpha:\n    print(\"We reject the null hypothesis. The variances are not similar.\")\nelse:\n    print(\"We fail to reject the null hypothesis. The variances are similar.\")\n\nWe reject the null hypothesis. The variances are not similar.\n\n\nVolvamos sobre los requisitos del test:\n\n¿Tenemos una sample representativa de la population? Suponemos que sí.\n¿Los datos son continuos? Sí.\n¿Las muestras siguen una distribución normal o hay más de 15 observaciones?\n\nNo, las muestras no siguen una distribución normal. Pero hay más de 15 observaciones en cada grupo, gracias al teorema central del límite podemos renunciar al supuesto de normalidad.\n\n¿Los grupos son independientes? Sí.\n¿Las varianzas son iguales (o al menos similares)?\n\nNo, las varianzas no son similares.\n\n\nLas varianzas no son similares. Vamos a realizar un test tipo Welch’s t-test.\nPara realizar el test utilizaremos scipy.stats.ttest_ind. Es necesario definir el parámetro equal_varbool como False.\n\n# Perform the Welch's t-test (equal_var=False)\nstatistic, p_value = stats.ttest_ind(\n    nonfiction_bestsellers[\"Reviews\"], fiction_bestsellers[\"Reviews\"], equal_var=False\n)\n\n# Print the results\nprint(\"p-value:\", p_value)\n\n# Check for significance\nalpha = 0.05  # Significance level\nif p_value &lt; alpha:\n    print(\n        \"We reject the null hypothesis: There are significant differences between the groups.\"\n    )\nelse:\n    print(\n        \"We cannot reject the null hypothesis: There are no significant differences between the groups.\"\n    )\n\np-value: 4.3970747273288255e-07\nWe reject the null hypothesis: There are significant differences between the groups.\n\n\n\n\n\nIdeas destacadas\nPodemos afirmar que hay diferencias significativas en términos de número de “Reviews” entre los libros de ficción y los de no ficción. Los libros del género de ficción obtienen, en media, más Reviews que los libros de no ficción.\n\n\n\n4.3 ¿Los géneros difieren en términos de “Price”?\nPrimero hagamos una exploración visual.\n\nfig = px.violin(\n    bestsellers,\n    x=\"Price\",\n    color=\"Genre\",\n    box=True,\n    points=\"outliers\",\n)\n\nfig.update_layout(\n    title_text=\"Price violin plot by Genre\",\n)\n\nfig.show()\n\n\n                                                \n\n\nEn esta ocasión “Price” podría no variar en función del género. ¿Pero es estadísticamente significativo? Vamos a comprobarlo con un test de hipótesis.\n\n1. Seleccionamos la hipotesis y el nivel de significancia\nH0: El precio medio de los libros de ficción es igual que el de los libros de no ficción\nHa: El precio medio de los libros de ficción es diferente que el de los libros de no ficción\nalpha = 0.05\n\n\n2. Identificamos el tipo de test\nDeseamos comparar las medias de dos grupos de nuestra sample. Por lo tanto, un 2-Sample t-Test parece ser adecuado. Lo primero es verificar si se cumplen los requisitos del test.\n\n2.1 Requisitos del test\n\nTenemos una sample representativa de la population.\nLos datos son continuos.\nLas muestras siguen una distribución normal o hay más de 15 observaciones.\nLos grupos son independientes.\nLas varianzas son iguales (o al menos similares).\n\nExaminemos nuestra sample para ver si podemos aplicar el test.\n\n\n\n\n\n\nLibro de refencia\n\n\n\n\n\nBook: Hypothesis Testing An Intuitive Guide For Making Data Driven Decisions\nPage: 48\nSection: 2-Sample t-Tests\n\n\n\nRealizamos un test de normalidad para cada grupo:\n\nresult_fiction = shapiro_test(fiction_bestsellers[\"Price\"])\nresult_nonfiction = shapiro_test(nonfiction_bestsellers[\"Price\"])\n\nprint(\"Para bestsellers de ficción:\", result_fiction)\nprint(\"Para bestsellers de no ficción:\", result_nonfiction)\n\nPara bestsellers de ficción: We reject the null hypothesis. The data does not follow a normal distribution.\nPara bestsellers de no ficción: We reject the null hypothesis. The data does not follow a normal distribution.\n\n\nEstudiemos la relación entre las varianzas de cada grupo.\n\n# Perform the Levene's Test\nstatistic, p_value = stats.levene(\n    fiction_bestsellers[\"Price\"], nonfiction_bestsellers[\"Price\"]\n)\n\n# Significance level\nalpha = 0.05\n\n# Check for significance\nif p_value &lt; alpha:\n    print(\"We reject the null hypothesis. The variances are not similar.\")\nelse:\n    print(\"We fail to reject the null hypothesis. The variances are similar.\")\n\nWe fail to reject the null hypothesis. The variances are similar.\n\n\nVolvamos sobre los requisitos del test:\n\n¿Tenemos una sample representativa de la population? Suponemos que sí.\n¿Los datos son continuos? Sí.\n¿Las muestras siguen una distribución normal o hay más de 15 observaciones?\n\nNo, las muestras no siguen una distribución normal. Pero hay más de 15 observaciones en cada grupo, gracias al teorema central del límite podemos renunciar al supuesto de normalidad.\n\n¿Los grupos son independientes? Sí.\n¿Las varianzas son iguales (o al menos similares)? Sí.\n\nSe cumplen los requisitos para realizar un 2-Sample t-Test.\nPara realizar el test vamos a usar scipy.stats.ttest_ind.\n\n# Perform the t-test\nstatistic, p_value = stats.ttest_ind(\n    nonfiction_bestsellers[\"Price\"], fiction_bestsellers[\"Price\"]\n)\n\n# Print the results\nprint(\"p-value:\", p_value)\n\n# Check for significance\nalpha = 0.05  # Significance level\nif p_value &lt; alpha:\n    print(\n        \"We reject the null hypothesis: There are significant differences between the groups.\"\n    )\nelse:\n    print(\n        \"We cannot reject the null hypothesis: There are no significant differences between the groups.\"\n    )\n\np-value: 0.1398175523683507\nWe cannot reject the null hypothesis: There are no significant differences between the groups.\n\n\n\n\n\nIdeas destacadas\nNo encontramos diferencias en el precio en función del género del libro."
  },
  {
    "objectID": "posts/2023-10-16-bestsellers_hypothesis_testing/bestsellers_with_categories_hypothesis_testing.html#recapitulación-y-reflexiones",
    "href": "posts/2023-10-16-bestsellers_hypothesis_testing/bestsellers_with_categories_hypothesis_testing.html#recapitulación-y-reflexiones",
    "title": "Test de Hipótesis: Análisis de los 50 libros más vendidos en amazon (2009 - 2019)",
    "section": "5. Recapitulación y reflexiones",
    "text": "5. Recapitulación y reflexiones\nDurante la preparación de los datos:\n\nHemos realizado una limpieza en el dataset, reduciendo el número de muestras para el estudio de 549 a 331.\nHemos descubierto que user rating concentra sus muestras en 10 valores en lugar de los 50 esperados.\n\nDurante el estudio mediante test de hipótesis:\n\nHemos mostrado con significancia estadística que el género del libro influye tanto en el número de reviews como en el user rating.\nHemos mostrado con significancia estadística que el género del libro no influye en el precio del mismo."
  },
  {
    "objectID": "posts/2023-11-20-credit_card_fraud_detection_2023/credit_card_fraud_detection_2023.html",
    "href": "posts/2023-11-20-credit_card_fraud_detection_2023/credit_card_fraud_detection_2023.html",
    "title": "Inteligencia Artificial contra el Fraude: Análisis de transacciones de tarjetas de crédito",
    "section": "",
    "text": "From Kaggle: Credit Card Fraud Detection Dataset 2023\n\nThis dataset contains credit card transactions made by European cardholders in the year 2023. It comprises over 550,000 records, and the data has been anonymized to protect the cardholders’ identities. The primary objective of this dataset is to facilitate the development of fraud detection algorithms and models to identify potentially fraudulent transactions. – From Kaggle’s dataset page\n\n\n\nTenemos un dataset anonimizado con transacciones de tarjetas de crédito realizadas en 2023. Algunas de estas transacciones han sido etiquetadas como fraudulentas. Una transacción se considera fraudulenta cuando ha sido realizada sin el consentimiento del dueño de la tarjeta, como ocurriría tras un robo.\n\n\n\nDesarrollar un modelo capaz de detectar las transacciones fraudulentas."
  },
  {
    "objectID": "posts/2023-11-20-credit_card_fraud_detection_2023/credit_card_fraud_detection_2023.html#a.-entendimiento-del-negocio",
    "href": "posts/2023-11-20-credit_card_fraud_detection_2023/credit_card_fraud_detection_2023.html#a.-entendimiento-del-negocio",
    "title": "Inteligencia Artificial contra el Fraude: Análisis de transacciones de tarjetas de crédito",
    "section": "",
    "text": "From Kaggle: Credit Card Fraud Detection Dataset 2023\n\nThis dataset contains credit card transactions made by European cardholders in the year 2023. It comprises over 550,000 records, and the data has been anonymized to protect the cardholders’ identities. The primary objective of this dataset is to facilitate the development of fraud detection algorithms and models to identify potentially fraudulent transactions. – From Kaggle’s dataset page\n\n\n\nTenemos un dataset anonimizado con transacciones de tarjetas de crédito realizadas en 2023. Algunas de estas transacciones han sido etiquetadas como fraudulentas. Una transacción se considera fraudulenta cuando ha sido realizada sin el consentimiento del dueño de la tarjeta, como ocurriría tras un robo.\n\n\n\nDesarrollar un modelo capaz de detectar las transacciones fraudulentas."
  },
  {
    "objectID": "posts/2023-11-20-credit_card_fraud_detection_2023/credit_card_fraud_detection_2023.html#b.-entendimiento-de-los-datos",
    "href": "posts/2023-11-20-credit_card_fraud_detection_2023/credit_card_fraud_detection_2023.html#b.-entendimiento-de-los-datos",
    "title": "Inteligencia Artificial contra el Fraude: Análisis de transacciones de tarjetas de crédito",
    "section": "1b. Entendimiento de los datos",
    "text": "1b. Entendimiento de los datos\nPara empezar, importamos las librerías que vamos a utilizar:\n\nPandas: Pandas es una biblioteca esencial en la ciencia de datos que proporciona estructuras de datos flexibles y eficientes, como DataFrames, para el análisis y manipulación de datos tabulares. Es ampliamente utilizada para limpiar, transformar y analizar datos, lo que la convierte en una herramienta fundamental para la preparación de datos en proyectos de ciencia de datos.\nScipy: Scipy es una biblioteca que se construye sobre NumPy y ofrece una amplia variedad de módulos y funciones especializadas para aplicaciones científicas y matemáticas. Incluye herramientas para estadísticas, optimización, álgebra lineal y procesamiento de señales, lo que la hace esencial en la investigación y el análisis de datos en ciencia de datos.\nScikit-learn (sklearn): Es una biblioteca clave para el aprendizaje automático en Python, ofreciendo una amplia variedad de algoritmos y herramientas para clasificación, regresión y agrupamiento.\nPlotly Express: Plotly Express es una biblioteca de visualización de datos que simplifica la creación de gráficos interactivos y visuales. Es especialmente útil en la exploración de datos y la comunicación de resultados en ciencia de datos, permitiendo a los científicos de datos crear visualizaciones informativas y atractivas con facilidad.\n\n\n\nCode\n# Import libraries\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import cross_validate\nimport time\n\n# Import plotly and customize\nimport plotly.io as pio\nimport plotly.express as px\n\npio.templates.default = \"plotly\"\npio.templates[\"plotly\"].layout.colorway = px.colors.qualitative.Set2\n\n\n\ndf = pd.read_csv(\"creditcard_2023.csv\")\n\n\n\n\n\n\n\nMatemáticas y código: en Inglés\n\n\n\n\n\nAunque el texto principal está en español, los términos matemáticos, los títulos de secciones y el código están en inglés.\nEsta práctica sigue el estándar internacional y ayuda a familiarizarse con el lenguaje técnico más utilizado en el campo de la ciencia de datos.\n\n\n\n\n1b.1 Descripción de datos\nEl dataset consiste en un archivo csv con las siguientes columnas:\n\nid: Identificador único para cada transacción\nV1-V28: funciones anónimas que representan varios atributos de transacción (por ejemplo, hora, ubicación, etc.)\nAmount: El monto de la transacción\nClass: Etiqueta binaria que indica si la transacción es fraudulenta (1) o no (0)\n\nNo se nos aporta información de las unidades, más allá de Amount, que probablemente sean euros.\n\ndf.describe()\n\n\n\n\n\n\n\n\nid\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\n...\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\nClass\n\n\n\n\ncount\n568630.000000\n5.686300e+05\n5.686300e+05\n5.686300e+05\n5.686300e+05\n5.686300e+05\n5.686300e+05\n5.686300e+05\n5.686300e+05\n5.686300e+05\n...\n5.686300e+05\n5.686300e+05\n5.686300e+05\n5.686300e+05\n5.686300e+05\n5.686300e+05\n5.686300e+05\n5.686300e+05\n568630.000000\n568630.0\n\n\nmean\n284314.500000\n-5.638058e-17\n-1.319545e-16\n-3.518788e-17\n-2.879008e-17\n7.997245e-18\n-3.958636e-17\n-3.198898e-17\n2.109273e-17\n3.998623e-17\n...\n4.758361e-17\n3.948640e-18\n6.194741e-18\n-2.799036e-18\n-3.178905e-17\n-7.497417e-18\n-3.598760e-17\n2.609101e-17\n12041.957635\n0.5\n\n\nstd\n164149.486122\n1.000001e+00\n1.000001e+00\n1.000001e+00\n1.000001e+00\n1.000001e+00\n1.000001e+00\n1.000001e+00\n1.000001e+00\n1.000001e+00\n...\n1.000001e+00\n1.000001e+00\n1.000001e+00\n1.000001e+00\n1.000001e+00\n1.000001e+00\n1.000001e+00\n1.000001e+00\n6919.644449\n0.5\n\n\nmin\n0.000000\n-3.495584e+00\n-4.996657e+01\n-3.183760e+00\n-4.951222e+00\n-9.952786e+00\n-2.111111e+01\n-4.351839e+00\n-1.075634e+01\n-3.751919e+00\n...\n-1.938252e+01\n-7.734798e+00\n-3.029545e+01\n-4.067968e+00\n-1.361263e+01\n-8.226969e+00\n-1.049863e+01\n-3.903524e+01\n50.010000\n0.0\n\n\n25%\n142157.250000\n-5.652859e-01\n-4.866777e-01\n-6.492987e-01\n-6.560203e-01\n-2.934955e-01\n-4.458712e-01\n-2.835329e-01\n-1.922572e-01\n-5.687446e-01\n...\n-1.664408e-01\n-4.904892e-01\n-2.376289e-01\n-6.515801e-01\n-5.541485e-01\n-6.318948e-01\n-3.049607e-01\n-2.318783e-01\n6054.892500\n0.0\n\n\n50%\n284314.500000\n-9.363846e-02\n-1.358939e-01\n3.528579e-04\n-7.376152e-02\n8.108788e-02\n7.871758e-02\n2.333659e-01\n-1.145242e-01\n9.252647e-02\n...\n-3.743065e-02\n-2.732881e-02\n-5.968903e-02\n1.590123e-02\n-8.193162e-03\n-1.189208e-02\n-1.729111e-01\n-1.392973e-02\n12030.150000\n0.5\n\n\n75%\n426471.750000\n8.326582e-01\n3.435552e-01\n6.285380e-01\n7.070047e-01\n4.397368e-01\n4.977881e-01\n5.259548e-01\n4.729905e-02\n5.592621e-01\n...\n1.479787e-01\n4.638817e-01\n1.557153e-01\n7.007374e-01\n5.500147e-01\n6.728879e-01\n3.340230e-01\n4.095903e-01\n18036.330000\n1.0\n\n\nmax\n568629.000000\n2.229046e+00\n4.361865e+00\n1.412583e+01\n3.201536e+00\n4.271689e+01\n2.616840e+01\n2.178730e+02\n5.958040e+00\n2.027006e+01\n...\n8.087080e+00\n1.263251e+01\n3.170763e+01\n1.296564e+01\n1.462151e+01\n5.623285e+00\n1.132311e+02\n7.725594e+01\n24039.930000\n1.0\n\n\n\n\n8 rows × 31 columns\n\n\n\nTodas las columnas anónimas tienen una standard deviation de 1 y una mean centrada 0. Esto nos indica que han sido normalizadas.\n\ndf[\"Class\"].value_counts()\n\nClass\n0    284315\n1    284315\nName: count, dtype: int64\n\n\nExiste el mismo número de muestras para cada clase.\n\n\nIdeas destacadas\n\nLas variables independientes han sido, como mínimo, normalizadas.\nExiste el exactamente el mismo número de muestras para cada clase. Es un dataset perfectamente balanceado.\n\nPodemos estar ante un dataset generado, en lugar de obtenido del mundo real. ¿Por qué?\n\nEs muy difícil acceder a un dataset con más de 2500000 muestras donde realmente se haya identificado una transacción fraudulenta.\nLa falta de información especifica sobre la procedencia de los datos.\n\nEsto es no es un problema. Generar datos es una opción válida (y a veces la única), existen ciertos escenarios donde la recolección de datos reales conlleva manejar datos sensibles. El único inconveniente estaría que el dataset se ha publicado como: “Transacciones con tarjeta de crédito realizadas por titulares de tarjetas europeos en el año 2023.”. Es decir, se da ha entender que se tratan de datos reales (en los que se ha anonimizado la información sensible)."
  },
  {
    "objectID": "posts/2023-11-20-credit_card_fraud_detection_2023/credit_card_fraud_detection_2023.html#a.-preparación-de-los-datos",
    "href": "posts/2023-11-20-credit_card_fraud_detection_2023/credit_card_fraud_detection_2023.html#a.-preparación-de-los-datos",
    "title": "Inteligencia Artificial contra el Fraude: Análisis de transacciones de tarjetas de crédito",
    "section": "2a. Preparación de los datos",
    "text": "2a. Preparación de los datos\n\n2a.1 Typecasting\nComprobamos el tipo de datos de las columnas y los modificamos conforme nuestra descripción inicial.\n\ndf.dtypes\n\nid          int64\nV1        float64\nV2        float64\nV3        float64\nV4        float64\nV5        float64\nV6        float64\nV7        float64\nV8        float64\nV9        float64\nV10       float64\nV11       float64\nV12       float64\nV13       float64\nV14       float64\nV15       float64\nV16       float64\nV17       float64\nV18       float64\nV19       float64\nV20       float64\nV21       float64\nV22       float64\nV23       float64\nV24       float64\nV25       float64\nV26       float64\nV27       float64\nV28       float64\nAmount    float64\nClass       int64\ndtype: object\n\n\n\ncategorical_columns = [\"Class\"]\ndf[categorical_columns] = df[categorical_columns].astype(\"category\")\n\n\n\n2a.2 Manejo de duplicados\nComprobamos si existen duplicados\n\ndf.duplicated().sum()\n\n0\n\n\n\n\n2a.3 Análisis de valores atípicos\n\nfor column in df.columns[df.columns.str.startswith(\"V\")]:\n    Q1 = df[column].quantile(0.25)\n    Q3 = df[column].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n\n    outliers = df[(df[column] &lt; lower_bound) | (df[column] &gt; upper_bound)][column]\n    print(f\"{column}: {len(outliers)} outliers\")\n\nV1: 30 outliers\nV2: 43620 outliers\nV3: 4300 outliers\nV4: 2766 outliers\nV5: 71015 outliers\nV6: 48854 outliers\nV7: 55997 outliers\nV8: 93856 outliers\nV9: 18939 outliers\nV10: 10710 outliers\nV11: 709 outliers\nV12: 700 outliers\nV13: 4197 outliers\nV14: 1088 outliers\nV15: 6780 outliers\nV16: 4546 outliers\nV17: 14890 outliers\nV18: 16676 outliers\nV19: 19519 outliers\nV20: 65708 outliers\nV21: 57138 outliers\nV22: 13706 outliers\nV23: 72056 outliers\nV24: 2029 outliers\nV25: 28089 outliers\nV26: 6554 outliers\nV27: 82235 outliers\nV28: 70242 outliers\n\n\n\nbox_plot = px.box(\n    df, y=\"Amount\", color=\"Class\", title=\"Box Plot of Transaction Amounts\"\n)\nbox_plot.for_each_trace(\n    lambda t: t.update(name=\"Fraudulent\" if t.name == \"1\" else \"Legitimate\")\n)\n\nbox_plot.show()\n\n\n                                                \n\n\n\nIdeas destacadas\nPara las variables anónimas, hemos localizado gran cantidad de outliers, como cabría esperar. Se pueden hacer tres cosas con los outliers, siguiendo la mnemotecnia 3R: rectificar, retener o remover. En este caso, podemos estar ante valores atípicos genuinos, por tanto vamos a retenerlos.\nEn cuanto a la variable “Amount”, no existen outliers. Además hay una simetría perfecta entre ambas clases, lo que parece confirmar nuestra suposición previa sobre que estamos trabajando con datos generados.\n\n\n\n2a.4 Variables con varianza cercana a cero\nNo existen variables con Zero & Near Zero Variance\n\n\n2a.5 Valores ausentes o faltantes\n\ndf.isnull().sum().sum()\n\n0\n\n\nNo existen Missing Values."
  },
  {
    "objectID": "posts/2023-11-20-credit_card_fraud_detection_2023/credit_card_fraud_detection_2023.html#b.-análisis-exploratorio-de-datos",
    "href": "posts/2023-11-20-credit_card_fraud_detection_2023/credit_card_fraud_detection_2023.html#b.-análisis-exploratorio-de-datos",
    "title": "Inteligencia Artificial contra el Fraude: Análisis de transacciones de tarjetas de crédito",
    "section": "2b. Análisis exploratorio de datos",
    "text": "2b. Análisis exploratorio de datos\nExploremos la correlación entre variables.\n\nfigure = plt.figure(figsize=[20, 10])\nsns.heatmap(df.corr(), cmap=\"crest\", annot=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nCorrelaciones con “Class”: algunas variables ‘V’ muestran correlaciones moderadas con la variable ‘Class’, que representa si una transacción es fraudulenta o no. Estas variables pueden ser muy importantes para predecir el fraude.\nLa variable ’Amount: no muestra correlaciones fuertes con la mayoría de las otras variables, lo que sugiere que el monto de las transacciones no está directamente relacionado con las otras características en términos de correlación lineal.\nCorrelaciones fuertes: se observan algunas correlaciones fuertes entre las variables ‘V’. Esto puede indicar relaciones subyacentes significativas que podrían ser exploradas para comprender mejor la estructura de los datos.\nDado que algunas variables están altamente correlacionadas, podrían ser candidatas para técnicas de reducción de dimensiones como PCA, para simplificar el modelo sin perder información valiosa.\n\nRepresentemos las variables con más correlación con la variable objetivo “Class”:\n\n# Calculate the correlation matrix\ncorrelation_matrix = df.corr()\n\n# Select correlations with 'Class' variable excluding 'Class' itself and 'ID'\nclass_correlations = correlation_matrix[\"Class\"].drop([\"Class\", \"id\"])\n\n# Sort the variables by their absolute correlation with 'Class'\nsorted_correlations = class_correlations.abs().sort_values(ascending=False)\n\n\nsorted_correlations.head(7)\n\nV14    0.805669\nV12    0.768579\nV4     0.735981\nV11    0.724278\nV3     0.682095\nV10    0.673665\nV9     0.585522\nName: Class, dtype: float64\n\n\n\n# Select the variables with the highest correlation\nselected_variables = sorted_correlations.head(4).index.tolist()\n\nprint(f\"selected_variables = {selected_variables}\")\n\n# A stratified sample is created for the charts\nsample_df, _ = train_test_split(df, train_size=1000, stratify=df[\"Class\"])\n\nselected_variables = ['V14', 'V12', 'V4', 'V11']\n\n\n\ndef generate_violin_plots(data, variables):\n    for variable in variables:\n        fig = px.violin(\n            data,\n            x=variable,\n            color=\"Class\",\n            box=True,\n            points=\"outliers\",\n            title=f\"Violin Plot for {variable}\",\n        )\n\n        fig.for_each_trace(\n            lambda t: t.update(name=\"Fraudulent\" if t.name == \"1\" else \"Legitimate\")\n        )\n        fig.show()\n\n\ngenerate_violin_plots(sample_df, selected_variables)\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n# Create the scatter matrix\nfig = px.scatter_matrix(\n    sample_df,\n    dimensions=selected_variables,\n    color=\"Class\",\n    title=\"Scatter Matrix of Selected Variables\",\n)\n\nfig.for_each_trace(\n    lambda t: t.update(name=\"Fraudulent\" if t.name == \"1\" else \"Legitimate\")\n)\nfig.show()\n\n\n                                                \n\n\n\nIdeas destacadas\nCon las 28 variables anónimas, un estudio detallado de centralidad, dispersión, skewness y kurtosis no parece lo más adecuado. Recapitulando:\n\nAlgunas variables ‘V’ tienen correlaciones moderadas con la variable ‘Class’, lo que es importante para predecir el fraude.\nSe observan correlaciones fuertes entre algunas variables ‘V’, lo que sugiere relaciones significativas. Podemos considerar la reducción de dimensionalidad.\nLa variable ‘Amount’ no tiene correlaciones fuertes con otras variables."
  },
  {
    "objectID": "posts/2023-11-20-credit_card_fraud_detection_2023/credit_card_fraud_detection_2023.html#c.-ingeniería-de-características",
    "href": "posts/2023-11-20-credit_card_fraud_detection_2023/credit_card_fraud_detection_2023.html#c.-ingeniería-de-características",
    "title": "Inteligencia Artificial contra el Fraude: Análisis de transacciones de tarjetas de crédito",
    "section": "2c. Ingeniería de características",
    "text": "2c. Ingeniería de características\nVamos a construir varios conjuntos de datos y evaluaremos el rendimiento de los modelos. Utilizaremos 4 conjuntos:\n\nEl conjunto original\nUn conjunto Threshold-Based\nDos conjuntos PCA\n\nLos creamos a continuación:\n\n2c.1 Feature Selection\nCreamos el conjunto Threshold-Base seleccionando las variables cuyo valor absoluto en la correlación con “Class” sea mayor a 0.6\n\ncorrelation_df = df[\n    sorted_correlations[sorted_correlations &gt; 0.6].index.tolist() + [\"Class\"]\n]\n\n\ncorrelation_df\n\n\n\n\n\n\n\n\nV14\nV12\nV4\nV11\nV3\nV10\nClass\n\n\n\n\n0\n0.549020\n0.293438\n-0.083724\n-0.987020\n2.496266\n0.637735\n0\n\n\n1\n0.627719\n1.564246\n-0.429654\n0.140107\n0.558056\n0.529808\n0\n\n\n2\n0.616874\n0.659201\n-0.457986\n-0.272985\n1.728538\n0.690708\n0\n\n\n3\n0.559535\n0.737483\n-1.090178\n-0.752581\n1.746840\n0.575231\n0\n\n\n4\n0.241454\n1.029577\n-0.448293\n-1.203171\n1.527053\n0.968046\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n568625\n-1.363454\n-1.330500\n0.904227\n1.430971\n-0.899794\n-1.161847\n1\n\n\n568626\n-0.812043\n-0.539948\n-0.267328\n0.173590\n-0.068129\n-0.210817\n1\n\n\n568627\n-0.395126\n-0.461849\n-0.035893\n0.477582\n0.137526\n-0.144495\n1\n\n\n568628\n-0.390369\n0.335215\n-0.144480\n-0.324934\n-0.300889\n-0.080078\n1\n\n\n568629\n-1.062488\n-1.153011\n0.374732\n0.892136\n-0.649140\n-0.513556\n1\n\n\n\n\n568630 rows × 7 columns\n\n\n\n\n\n2c.2 Feature Extraction\nRealizamos Principal Component Analysis. Creamos dos nuevos conjuntos de datos.\n\n# Select the features to be analyzed, which are columns \"V1\" to \"V28\"\nfeatures = [f\"V{i}\" for i in range(1, 29)]\n\n# Extract the selected features from the DataFrame 'df'\nx = df.loc[:, features].values\n\n# Perform Principal Component Analysis (PCA) with 2 components\npca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(x)\n\n# Create a new DataFrame to store the principal components\nprincipalDf = pd.DataFrame(data=principalComponents, columns=[\"pc1\", \"pc2\"])\n\n# Concatenate the principal components with the 'Class' column\npca_2_df = pd.concat([principalDf, df[[\"Class\"]]], axis=1)\n\n\n# Create a stratified sample with 1000 data points for visualization\nsample_df, _ = train_test_split(pca_2_df, train_size=1000, stratify=df[\"Class\"])\n\n# Create a scatter plot using Plotly Express to visualize the principal components\nfig = px.scatter(sample_df, x=\"pc1\", y=\"pc2\", color=\"Class\")\n\n# Update the legend names for clarity\nfig.for_each_trace(\n    lambda t: t.update(name=\"Fraudulent\" if t.name == \"1\" else \"Legitimate\")\n)\nfig.show()\n\n\n                                                \n\n\n\n# Perform Principal Component Analysis (PCA) with 6 components\npca = PCA(n_components=6)\nprincipalComponents = pca.fit_transform(x)\n\n# Create a new DataFrame to store the principal components\nprincipalDf = pd.DataFrame(\n    data=principalComponents, columns=[f\"pc{i}\" for i in range(1, 7)]\n)\n\n# Concatenate the principal components with the 'Class' column\npca_6_df = pd.concat([principalDf, df[[\"Class\"]]], axis=1)\n\n\n# Create a stratified sample with 1000 data points for visualization\nsample_df, _ = train_test_split(pca_6_df, train_size=1000, stratify=df[\"Class\"])\n\n# Create the scatter matrix\nfig = px.scatter_matrix(\n    sample_df,\n    dimensions=[f\"pc{i}\" for i in range(1, 7)],\n    color=\"Class\",\n    title=\"Scatter Matrix of Selected Variables\",\n)\n\nfig.for_each_trace(\n    lambda t: t.update(name=\"Fraudulent\" if t.name == \"1\" else \"Legitimate\")\n)\nfig.show()\n\n\n                                                \n\n\n\noriginal_df = df.drop(columns=[\"id\"])\n\n# Assign the scaled values back to the \"Amount\" column in the original DataFrame\namount_df = original_df[[\"Amount\"]]\nsc = StandardScaler()\nscaled_amount = sc.fit_transform(amount_df)\noriginal_df[\"Amount\"] = scaled_amount\n\n\n\nIdeas destacadas\nVamos a comparar el rendimiento que podemos obtener con los siguientes datasets:\n\ncorrelation_df, con las seis variables que tienen una correlación mayor a 0.6\npca_2_df, con dos componentes principales de PCA\npca_6_df, con seis componentes principales de PCA\noriginal_df, el conjunto original (con 29 variables)"
  },
  {
    "objectID": "posts/2023-11-20-credit_card_fraud_detection_2023/credit_card_fraud_detection_2023.html#modelos",
    "href": "posts/2023-11-20-credit_card_fraud_detection_2023/credit_card_fraud_detection_2023.html#modelos",
    "title": "Inteligencia Artificial contra el Fraude: Análisis de transacciones de tarjetas de crédito",
    "section": "3. Modelos",
    "text": "3. Modelos\nEmpecemos con los siguientes modelos de aprendizaje automático superficial:\n\nLogistic Regression\nDecision Tree\nK-Nearest Neighbors\nNaive Bayes\n\n\n3.1 Modelos de aprendizaje automático superficial (Shallow Machine Learning Models)\n\n# Create a bar plot to visualize the results, grouped by dataset and model\nfig = px.bar(\n    results_shallow_models_df,\n    x=\"Dataset\",\n    y=[\"CV Accuracy\", \"CV Precision\", \"CV Recall\", \"CV F1 Score\"],\n    barmode=\"group\",\n    facet_col=\"Model\",\n)\n\n# Configura el rango del eje y entre 0.8 y 1\nfig.update_yaxes(range=[0.8, 1])\n\nfig.show()\n\n\n                                                \n\n\n\n\n\n\n\n\nFíjate que las gráficas se presentan con ZOOM en el eje Y\n\n\n\nEl valor más bajo de accuracy, que estamos viendo en las gráficas, está por encima del 90%\n\n\n\n\n3.2 Random Forest with Hypertuning and Principal Component Analysis (PCA) with 3 components\nLos resultados obtenidos con el dataset pca_2_df son muy buenos. Tenemos que recordar que en este conjunto de datos solo tenemos dos columnas.\nVamos a crear un dataset pero con 3 PCA y entrenar un Random Forest con selección de hiperparámetros.\n\n# Perform Principal Component Analysis (PCA) with 3 components\npca = PCA(n_components=3)\nprincipalComponents = pca.fit_transform(x)\n\n# Create a new DataFrame to store the principal components\nprincipalDf = pd.DataFrame(\n    data=principalComponents, columns=[f\"pc{i}\" for i in range(1, 4)]\n)\n\n# Concatenate the principal components with the 'Class' column\npca_3_df = pd.concat([principalDf, df[[\"Class\"]]], axis=1)\n\n\npca_3_df.head()\n\n\n\n\n\n\n\n\npc1\npc2\npc3\nClass\n\n\n\n\n0\n2.190825\n-0.165314\n0.265893\n0\n\n\n1\n2.085009\n0.355739\n0.168428\n0\n\n\n2\n2.282798\n-0.521145\n-0.361626\n0\n\n\n3\n2.694661\n0.107729\n-0.255410\n0\n\n\n4\n2.048980\n-0.354799\n0.937397\n0\n\n\n\n\n\n\n\nAl trabajar con 3 dimensiones (variables) es muy interesante crear una representación gráfica interactiva.\n\n# Create a stratified sample with 5000 data points for visualization\nsample_df, _ = train_test_split(pca_3_df, train_size=5000, stratify=df[\"Class\"])\n\nfig = px.scatter_3d(\n    sample_df,\n    x=\"pc1\",\n    y=\"pc2\",\n    z=\"pc3\",\n    color=\"Class\",\n    color_discrete_map={0: \"blue\", 1: \"red\"},\n    labels={\"Class\": \"Transaction Type\"},\n    title=\"3D PCA Scatter Plot\",\n)\n\n# Update the legend names for clarity\nfig.for_each_trace(\n    lambda t: t.update(name=\"Fraudulent\" if t.name == \"1\" else \"Legitimate\")\n)\n\n\nfig.show()\n\n\n                                                \n\n\n\n\n\n\n\n\nLa gráfica es interactiva\n\n\n\nPuedes rotarla libremente en cualquier eje para entender mejor los datos.\n\n\n\n\nStarting hyperparameter search for Random Forest...\nFitting 3 folds for each of 4 candidates, totalling 12 fits\n[CV] END bootstrap=True, criterion=gini, max_depth=20, n_estimators=200; total time= 2.2min\n[CV] END bootstrap=False, criterion=gini, max_depth=20, n_estimators=200; total time= 3.1min\n[CV] END bootstrap=True, criterion=gini, max_depth=20, n_estimators=200; total time= 2.2min\n[CV] END bootstrap=True, criterion=gini, max_depth=20, n_estimators=500; total time= 5.4min\n[CV] END bootstrap=True, criterion=gini, max_depth=20, n_estimators=500; total time= 5.2min\n[CV] END bootstrap=False, criterion=gini, max_depth=20, n_estimators=200; total time= 3.1min\n[CV] END bootstrap=False, criterion=gini, max_depth=20, n_estimators=200; total time= 3.1min\n[CV] END bootstrap=True, criterion=gini, max_depth=20, n_estimators=200; total time= 2.2min\n[CV] END bootstrap=True, criterion=gini, max_depth=20, n_estimators=500; total time= 5.3min\n[CV] END bootstrap=False, criterion=gini, max_depth=20, n_estimators=500; total time= 7.0min\n[CV] END bootstrap=False, criterion=gini, max_depth=20, n_estimators=500; total time= 7.1min\n[CV] END bootstrap=False, criterion=gini, max_depth=20, n_estimators=500; total time= 7.0min\nHyperparameter search completed in: 25 minutes 22 seconds\n\n\n/home/mike/.anaconda3/envs/data_road/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning:\n\nA worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n\n\n\n\n# Display the results\nfor metric, value in results.items():\n    if metric != \"Best Parameters\":\n        print(f\"{metric}: {value:.3f}\")\n    else:\n        print(f\"{metric}: {value}\")\n\n# Convert the results into a DataFrame for tabular representation\nresults_random_forest_df = pd.DataFrame([results])\n\nBest Parameters: {'bootstrap': False, 'criterion': 'gini', 'max_depth': 20, 'n_estimators': 500}\nAccuracy: 0.990\nPrecision: 1.000\nRecall: 0.981\nF1 Score: 0.990\n\n\n\n\nIdeas destacadas\nLos modelos que mejor funcionan son:\n\nDecision Tree\nK-Nearest Neighbors\n\nLo más interesante nos lo muestran los datasets. Recordemos que variables contiene cada uno:\n\ncorrelation_df, con las seis variables que tienen una correlación mayor a 0.6\npca_2_df, con dos componentes principales de PCA\npca_6_df, con seis componentes principales de PCA\noriginal_df, el conjunto original (con 29 variables)\n\nQué hemos descubierto:\n\nPese a contar con 23 dimensiones menos, tanto correlation_df como pca_6_df logran unos resultados similares al dataset original. Eso significa que mediante las técnicas de reducción de dimensionalidad han funcionado correctamente logrando mantener la información relevante.\nLos resultados obtenidos con pca_2_df son muy buenos para haberlos obtenido mediante 2 dimensiones.\n\nPor último, hemos creado un dataset con 3 PCA y entrenado un modelo Random Forest (con selección de hiperparámetros). Este enfoque es el que mejor resultado nos ha entregado. Vamos a explorarlo a continuación."
  },
  {
    "objectID": "posts/2023-11-20-credit_card_fraud_detection_2023/credit_card_fraud_detection_2023.html#evaluación",
    "href": "posts/2023-11-20-credit_card_fraud_detection_2023/credit_card_fraud_detection_2023.html#evaluación",
    "title": "Inteligencia Artificial contra el Fraude: Análisis de transacciones de tarjetas de crédito",
    "section": "4. Evaluación",
    "text": "4. Evaluación\n\n4.1 Medidas de precisión del modelo\nComparemos la curva ROC de algunos de los modelos\n\n# Display the confusion matrix and classification report\nprint(f\"Confusion Matrix:\\n{conf_matrix}\\n\")\nprint(f\"Classification Report:\\n{class_report}\\n\")\n\n# Save the results in a DataFrame for tabular representation\nresults = {\n    \"Accuracy\": [accuracy],\n    \"Precision\": [precision],\n    \"Recall\": [recall],\n    \"F1 Score\": [f1],\n    \"ROC AUC\": [roc_auc_best],\n}\nresults_random_forest_df = pd.DataFrame(results)\n\nConfusion Matrix:\n[[84319   830]\n [ 3343 82097]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      0.99      0.98     85149\n           1       0.99      0.96      0.98     85440\n\n    accuracy                           0.98    170589\n   macro avg       0.98      0.98      0.98    170589\nweighted avg       0.98      0.98      0.98    170589\n\n\n\n\n\n# Create a DataFrame for each model\ndf_rf = pd.DataFrame(\n    {\n        \"False Positive Rate\": fpr_best,\n        \"True Positive Rate\": tpr_best,\n        \"Model\": \"Random Forest (area = %0.2f)\" % roc_auc_best,\n    }\n)\ndf_logistic = pd.DataFrame(\n    {\n        \"False Positive Rate\": fpr_logistic,\n        \"True Positive Rate\": tpr_logistic,\n        \"Model\": \"Logistic Regression Classifier (area = %0.2f)\" % roc_auc_logistic,\n    }\n)\ndf_tree = pd.DataFrame(\n    {\n        \"False Positive Rate\": fpr_gaussian_nb,\n        \"True Positive Rate\": tpr_gaussian_nb,\n        \"Model\": \"Gaussian Naive Bayes (area = %0.2f)\" % roc_auc_gaussian_nb,\n    }\n)\ndf_random = pd.DataFrame(\n    {\n        \"False Positive Rate\": [0, 1],\n        \"True Positive Rate\": [0, 1],\n        \"Model\": \"Random Classifier (area = 0.5)\",\n    }\n)\n\n# Concatenate the DataFrames\ndf = pd.concat([df_rf, df_logistic, df_tree, df_random])\n\n# Create a line chart with Plotly Express\nfig = px.line(\n    df,\n    x=\"False Positive Rate\",\n    y=\"True Positive Rate\",\n    color=\"Model\",\n    title=\"ROC Curve Comparison\",\n    markers=True,\n)\n\n# Add additional details to the chart\nfig.update_layout(\n    xaxis_title=\"False Positive Rate\",\n    yaxis_title=\"True Positive Rate\",\n    legend_title=\"Model\",\n    showlegend=True,\n)\n\n# Show the chart\nfig.show()"
  },
  {
    "objectID": "posts/2023-11-20-credit_card_fraud_detection_2023/credit_card_fraud_detection_2023.html#recapitulación-y-reflexiones",
    "href": "posts/2023-11-20-credit_card_fraud_detection_2023/credit_card_fraud_detection_2023.html#recapitulación-y-reflexiones",
    "title": "Inteligencia Artificial contra el Fraude: Análisis de transacciones de tarjetas de crédito",
    "section": "5. Recapitulación y reflexiones",
    "text": "5. Recapitulación y reflexiones\n\n5.1 Sobre los datos\nHemos descubierto que, aunque no se anuncia como tal, estamos trabajando con un conjunto de datos que ha sido generado (no recolectado). Esto no es un problema. Generar datos es una opción válida (y a veces la única); existen ciertos escenarios donde la recolección de datos reales conlleva manejar datos sensibles. El único inconveniente estaría en que el dataset se ha publicado como: “Transacciones con tarjeta de crédito realizadas por titulares de tarjetas europeos en el año 2023”. Es decir, se da a entender que se tratan de datos reales (en los que se ha anonimizado la información sensible).\nHemos descubierto que varias de las variables mantienen una relación lineal. También que la variable “Amount” aporta poca información sobre el tipo de transacción (cosa que sería poco habitual si estuviésemos trabajando con un dataset real).\n\n\n5.2 Sobre el rendimiento de los modelos\nHemos creado nuevos datasets reduciendo la dimensionalidad del original. Estos nuevos datasets se han construido seleccionando las seis componentes principales de PCA o las seis variables que tienen una correlación mayor a 0.6. La ejecución de los modelos sobre estos conjuntos de datos ha mostrado un rendimiento similar al obtenido al utilizar las 29 dimensiones del conjunto original.\nEl dataset creado únicamente con dos componentes principales de PCA ha arrojado unos resultados sorprendentes. Por ello, hemos creado un dataset con 3 PCA y entrenado un modelo Random Forest (con selección de hiperparámetros). Este enfoque es el que mejor resultado nos ha entregado, con un accuracy en torno al 99% y una superficie bajo la curva ROC cercana al 0.99.\nDe todos modos, cabe señalar que modelos más sencillos como Logistic Regression han logrado resultados muy competitivos con una complejidad inferior a Random Forest (con selección de hiperparámetros)."
  }
]